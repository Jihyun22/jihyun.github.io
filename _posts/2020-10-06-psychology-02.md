---
title: "[λ°μ΄μ½ μ‹¬λ¦¬μ„±ν–¥ μμΈ΅ λ€ν] AUTO ML - λ² μ΄μ§€μ• μµμ ν™” (Bayesian Optimization)"
date: 2020-10-06
categories: λ°μ΄μ½λ¦¬λ·°
toc: true
toc_sticky: true
toc_label: "λ©μ°¨"
tags : data dacon contest
related: true
header:
  teaser: "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/psychology.png?raw=true"
---



<img src= "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/psychology.png?raw=true">

μ›”κ°„λ°μ΄μ½ 8 μ‹¬λ¦¬ μ„±ν–¥ μμΈ΅ κ²½μ§„ λ€ν λ¦¬λ·° 2νΈμ…λ‹λ‹¤. 

- λ°μ΄μ½ λ€ν μ•λ‚΄ λ°”λ΅κ°€κΈ° [λ§ν¬](https://dacon.io/competitions/official/235647/overview/)
- [λ°μ΄μ½ μ‹¬λ¦¬μ„±ν–¥ μμΈ΅ λ€ν] λ¦¬λ·° 1νΈ λ°”λ΅κ°€κΈ° [λ§ν¬](jihyun22.github.io/λ°μ΄μ½λ¦¬λ·°/psychology-01/)

<center><p>
  <img src="https://visitor-badge.laobi.icu/badge?page_id=jihyun22.github.io/λ°μ΄μ½λ¦¬λ·°/psychology-02/" alt="visitor"/>
    </p></center>


---

<br/>



# π€ **κ°μ”** 

---

<br/>

μ΄λ² μ›”κ°„ λ°μ΄μ½μ μ‹¬λ¦¬ μ„±ν–¥ μμΈ΅ κ²½μ§„λ€νλ” λ§ν‚¤μ•„λ°Έλ¦¬μ¦ μ‹¬λ¦¬ ν…μ¤νΈλ¥Ό ν™μ©ν•μ—¬ ν…μ¤νΈ μ°Έκ°€μμ κµ­κ°€ μ„ κ±° ν¬ν‘ μ—¬λ¶€λ¥Ό μμΈ΅ν•λ” λ€νμ…λ‹λ‹¤.

1νΈ [λ°μ΄μ½ μ‹¬λ¦¬μ„±ν–¥ μμΈ΅ λ€ν] λ¬Έμμ—΄μ΄ ν¬ν•¨λ λ°μ΄ν„° μ²λ¦¬ν•κΈ° [λ§ν¬]((https://jihyun22.github.io/%EB%8D%B0%EC%9D%B4%EC%BD%98%EB%A6%AC%EB%B7%B0/psychology-01/))μ—μ„λ” Label encoding κ³Ό κ΄€λ ¨ν•μ—¬ `get_dummies()` λ¥Ό μ‚¬μ©ν• λ¬Έμμ—΄μ΄ ν¬ν•¨λ λ³€μλ¥Ό μ²λ¦¬ν•λ” λ°©λ²•μ— λ€ν•΄ λ‹¤λ£¨μ—μµλ‹λ‹¤. 

- [λ°μ΄μ½ μ‹¬λ¦¬μ„±ν–¥ μμΈ΅ λ€ν] λ¦¬λ·° 1νΈ λ°”λ΅κ°€κΈ° [λ§ν¬](jihyun22.github.io/λ°μ΄μ½λ¦¬λ·°/psychology-01/)

<br/>

μ΄λ² ν¬μ¤νΈμ—μ„λ” Auto ML λ°©λ²• μ¤‘ ν•μ΄νΌ νλΌλ―Έν„° νλ‹μ— λ€ν•΄ λ‹¤λ£¨λ©°,  Bayesian Optimization(λ² μ΄μ§€μ• μµμ ν™”) μ μ© λ°©λ²•μ„ μ†κ°ν•κ² μµλ‹λ‹¤. Bayesian Optimizationμ μμ„Έν• μ„¤λ…μ€ μ•„λ λ§ν¬λ¥Ό μ°Έκ³ ν•μ„Έμ”.

- [AUTO ML] λ² μ΄μ§€μ• μµμ ν™” (Bayesian Optimization) [λ§ν¬](jihyun22.github.io/automl/Bayesian Optimization/)




---

<br/>



# **01~2 data_encoding.ipynb μ°Έκ³ **
---

<br/>

μ‚¬μ© μ–Έμ–΄λ” `python` , κµ¬ν„ ν™κ²½μ€  `jupyter notebook` μ…λ‹λ‹¤.

```python
import pandas as pd
import numpy as np

# λ°μ΄ν„° λ΅λ“
train=pd.read_csv('data/train.csv', index_col=0)
test=pd.read_csv('data/test_x.csv', index_col=0)
submission=pd.read_csv('data/sample_submission.csv', index_col=0)
# λ²”μ£Όν• -> μ΄μ‚°ν•
X = pd.get_dummies(train.drop('voted', axis = 1))
y = train['voted']
test = pd.get_dummies(test)

# μΉΌλΌ κ°μ λ³€ν™”
print("X : {}\ntest : {}".format(X.shape, test.shape))
# μΈμ½”λ”© ν™•μΈ
print("Encoding Success") if list(X.columns) == list(test.columns) else list(test.columns)

#X : (45532, 100)
#test : (11383, 100)
#Encoding Success
```

λ°μ΄ν„°λ¥Ό λ¶λ¬μ¨ ν›„ μ΄μ „ κ²μ‹λ¬Όμ—μ„ λ‹¤λ£¨μ—λ μΈμ½”λ”©μ„ μ§„ν–‰ν•©λ‹λ‹¤.

<br/>

---

<br/>

# **03 λ°μ΄ν„° μ „μ²λ¦¬(2)**
---

<br/>

λ°μ΄ν„° λ‚΄ Nan κ°’κ³Ό μ¤‘λ³µκ°’μ„ μ²λ¦¬ν•©λ‹λ‹¤.

nan κ°’μ„ μ—΄μ ν‰κ· μΌλ΅ λ°”κΎΈλ” `fillna` λ©”μ†λ“μ™€ μ¤‘λ³µ λ°μ΄ν„°λ¥Ό μ²λ¦¬ν•λ” `drop_duplicates`λ¥Ό μ‚¬μ©ν•κ² μµλ‹λ‹¤.

<br/>

```python
# nan κ°’ λ©”κΎΈκΈ°
X = X.fillna(X.mean())
# μ¤‘λ³µ κ°’ μ κ±°
X.drop_duplicates(keep='first', inplace = True)
# λΉ„κµ -> nan μ—†μ
X.shape

# (45532, 100)
```

<br/>

λ‹¤μμΌλ΅ λ°μ΄ν„° μ¤μΌ€μΌλ§μ„ μ§„ν–‰ν•κ² μµλ‹λ‹¤. λ‹¤μ–‘ν• κ°’μ΄ μ΅΄μ¬ν•λ” λ°μ΄ν„°μ μµλ€κ°’κ³Ό μµμ†κ°’μ„ λ§μ¶”κ±°λ‚ ν‰κ· μ„ λ§μ¶”λ” μ‘μ—…μΌλ΅ μ‚¬μ΄ν‚·λ° ν¨ν‚¤μ§€λ΅ μ‰½κ² μ²λ¦¬ν•  μ μμµλ‹λ‹¤.

MinMaxScalerμ„ μ‚¬μ©ν•μ—¬ μ²λ¦¬ν•κ² μµλ‹λ‹¤.

```python
from sklearn.preprocessing import MinMaxScaler
# λ°μ΄ν„° μ¤μΌ€μΌλ§ -> λ―Όλ§¥μ¤/μ¤ν…λ‹¤λ“ λ¨λ‘ μ„±λ¥ λΉ„μ·ν•¨
scaler=MinMaxScaler()
scaler.fit(X)
X=scaler.transform(X)
# ν…μ¤νΈ λ°μ΄ν„°λ„ λ™μΌ μ¤μΌ€μΌλ¬λ΅
test=scaler.transform(test)
```

μ΄λ• μ¤‘μ”ν• μ μ€ train λ°μ΄ν„°μ™€ λ™μΌν• μ¤μΌ€μΌλ¬λ¥Ό testλ°μ΄ν„°μ— μ μ©ν•΄μ„ μ¤μΌ€μΌλ§ ν•΄μ•Ό ν•λ‹¤λ” μ μ…λ‹λ‹¤. μ‹¤μ λ΅ Standard μ¤μΌ€μΌλ§μ™€ Robust μ¤μΌ€μΌλ§λ„ μ§„ν–‰ν•μ€μΌλ‚ μ‹¤μ  κ²°κ³Όκ°’μ—λ” ν° μ°¨μ΄κ°€ μ—†μ—μµλ‹λ‹¤. μ„¤λ¬Έ μ΅°μ‚¬ κ²°κ³Όκ°€ μ…λ ¥λ λ°μ΄ν„° νΉμ„± μƒ λ°μ΄ν„°μ λ¶„ν¬κ°€ μΉΌλΌ λ³„λ΅ ν° μ°¨μ΄κ°€ μ—†κΈ° λ•λ¬ΈμΈ κ²ƒ κ°™μµλ‹λ‹¤.

λ”°λΌμ„ μ¤μΌ€μΌλ§ μ‘μ—…μ€ μƒλµν•΄λ„ λ¬΄λ°©ν•©λ‹λ‹¤. 

β€‹    <br/>

```python
X

#array([[5.00000000e-01, 1.40020340e-04, 7.50000000e-01, ...,
#        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],
#       [1.00000000e+00, 2.57670567e-04, 1.00000000e+00, ...,
#        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
#       [7.50000000e-01, 6.61989656e-04, 0.00000000e+00, ...,
#        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],
#       ...,
#       [7.50000000e-01, 2.35300453e-04, 0.00000000e+00, ...,
#        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
#       [0.00000000e+00, 2.99096703e-04, 5.00000000e-01, ...,
#        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
#       [5.00000000e-01, 1.95117101e-04, 1.00000000e+00, ...,
#        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])
```

μ „μ²λ¦¬κ°€ μ™„λ£λμ—μµλ‹λ‹¤.

  <br/>

---

  <br/>

# 04 λ¨λΈλ§ (BayesianOptimization)

  <br/>

Auto ML λ¶„μ•ΌμΈ Hyperparameter Optimization μ¤‘ bayesian optimization λ°©λ²•μ„ μ μ©ν•μ—¬ λ¨λΈλ§μ„ μ§„ν–‰ν•κ² μµλ‹λ‹¤.

λ¨λΈ μ•™μƒλΈ”μ„ μ μ™Έν•κ³  λ‹¨μΌ λ¨λΈλ΅ μµμ  κ²°κ³Όκ°’μ„ μ—°μ‚°ν•κ² μµλ‹λ‹¤. μ‚¬μ©ν•  λ¨λΈμ€ LGBMμ…λ‹λ‹¤.

bayesian-optimizationμ„ νμ΄μ¬ ν™κ²½μ—μ„ μ‚¬μ©ν•κΈ° μ„ν•΄μ„ ν•΄λ‹Ή ν¨ν‚¤μ§€λ¥Ό μ„¤μΉν•΄μ•Ό ν•©λ‹λ‹¤. pip λ…λ ΉμΌλ΅ μ„¤μΉν•  μ μμµλ‹λ‹¤.

-  bayesian-optimization 1.2.0 [λ§ν¬](https://pypi.org/project/bayesian-optimization/)

```python
pip install bayesian-optimization
```

`bayes_opt` λ¨λ“μ΄ ν™κ²½μ— λ‹¤μ΄λ΅λ“ λλ©° μ„¤μΉκ°€ μ™„λ£λ©λ‹λ‹¤.

  <br/>

ν•„μ”ν• ν¨ν‚¤μ§€λ¥Ό μ„ν¬νΈν•κ² μµλ‹λ‹¤. 

```python
import lightgbm as lgbm
from bayes_opt import BayesianOptimization
from sklearn.metrics import roc_auc_score, make_scorer
from sklearn.model_selection import cross_validate
```

μ‚¬μ©ν•  λ¨λΈμΈ `lgbm` μ™€ `BayesianOptimization` λ¨λ“μ„ κ°€μ Έμµλ‹λ‹¤. μ¶”κ°€λ΅  λ³Έ λ€νμ ν‰κ°€μ§€ν‘λ” AUCλ΅ auc_scoreμ„ κ³„μ‚°ν•κΈ° μ„ν•΄ μ‚¬μ΄ν‚·λ°μ `roc_auc_score` λ¨λ“ λ“±μ„ μ„ν¬νΈν–μµλ‹λ‹¤.

  <br/>

λ‹¤μμ€ λ©μ ν•¨μλ¥Ό μƒμ„±ν•΄μ•Ό ν•©λ‹λ‹¤. λ©μ ν•¨μλ€ lgbmμ μ„±λ¥ ν•¨μλ΅ λ¨λΈμ νλΌλ―Έν„° μ΅°ν•©μ„ μ…λ ¥κ°’μΌλ΅ κ°€μ§‘λ‹λ‹¤. μ„±λ¥ν‰κ°€λ” aucμ΄λ―€λ΅ scoreκ°’μ„ return ν•΄ μ¤λ‹λ‹¤. 

```python
#λ©μ ν•¨μ μƒμ„±
def lgbm_cv(learning_rate, num_leaves, max_depth, min_child_weight, colsample_bytree, feature_fraction, bagging_fraction, lambda_l1, lambda_l2):
    model = lgbm.LGBMClassifier(learning_rate=learning_rate,
                                n_estimators = 300,
                                num_leaves = int(round(num_leaves)),
                                max_depth = int(round(max_depth)),
                                min_child_weight = int(round(min_child_weight)),
                                colsample_bytree = colsample_bytree,
                                feature_fraction = max(min(feature_fraction, 1), 0),
                                bagging_fraction = max(min(bagging_fraction, 1), 0),
                                lambda_l1 = max(lambda_l1, 0),
                                lambda_l2 = max(lambda_l2, 0)
                               )
    scoring = {'roc_auc_score': make_scorer(roc_auc_score)}
    result = cross_validate(model, X, y, cv=5, scoring=scoring)
    auc_score = result["test_roc_auc_score"].mean()
    return auc_score
```

ν•¨μμ μΈμλ” λ¨λΈμ νλΌλ―Έν„°μ…λ‹λ‹¤. μµμ  κ°’μ„ νƒμƒ‰ν•κ³ μ ν•λ” νλ¦¬λ―Έν„°λ¥Ό μ…λ ¥ν•΄μ¤λ‹λ‹¤. μ €λ” μ΄ 9κ°μ νλΌλ―Έν„°λ΅ μ΅°ν•©μ„ κµ¬μ„±ν–μµλ‹λ‹¤. `n_estimators`μ€ ν•λ“κ°’μΌλ΅ κ³ μ •ν•κ³  λ‹¤λ¥Έ νλΌλ―Έν„°λ¥Ό μ΅°μ •ν•  κ³„νμ…λ‹λ‹¤.  `int(round())` μ€ intκ°’μ„ κ°€μ§€λ” νλΌλ―Έν„°μ— λ€ν•΄ μ„¤μ •ν•κ³ , μµλ€κ°’κ³Ό μµμ†κ°’μ΄ κ³ μ •λ νλΌλ―Έν„°λ” `max`μ™€ `min` ν•¨μλ΅ λ²”μ„λ¥Ό μ„¤μ •ν•©λ‹λ‹¤. 

`cross_validate`μ€ scoreμ„ κ³„μ‚°ν•λ” ν•¨μμ…λ‹λ‹¤. λ‹¨μΌ ν‰κ°€μ§€ν‘λ¥Ό μ‚¬μ©ν•λ” κ²½μ° `cross_val_score` μ„ μ‚¬μ©ν•λ”κ² μΌλ°μ μ΄μ§€λ§, λ‹¤μ¤‘ μ§€ν‘λ¥Ό μ‚¬μ©ν•  κ²½μ°μ— λ€λΉ„ν•΄μ„   `make_scorer` λ©”μ†λ“λ΅ roc_auc_score κ°’μ„ λ„μ¶ν•  μ μλ„λ΅ μ„¤μ •ν•μ€μµλ‹λ‹¤. λ§¤κ°λ³€μμΈ cvκ°’μ€ 5λ΅ μ§€μ •ν–μµλ‹λ‹¤. 

  <br/>

λ‹¤μμ€ μ…λ ¥κ°’, μ¦‰ νλΌλ―Έν„°μ νƒμƒ‰ κµ¬κ°„μ„ μ„¤μ •ν•©λ‹λ‹¤.

```python
# μ…λ ¥κ°’μ νƒμƒ‰ λ€μƒ κµ¬κ°„
pbounds = {'learning_rate' : (0.0001, 0.05),
           'num_leaves': (300, 600),
           'max_depth': (2, 25),
           'min_child_weight': (30, 100),
           'colsample_bytree': (0, 0.99),
           'feature_fraction': (0.0001, 0.99),
           'bagging_fraction': (0.0001, 0.99),
           'lambda_l1' : (0, 0.99),
           'lambda_l2' : (0, 0.99),
          }
```

νλΌλ―Έν„°μ νƒμƒ‰ λ²”μ„λ” λ¨λΈ λ³„ μ„¤λ… μλ£λ¥Ό μ°Έκ³ ν•μ—¬ μ„¤μ •ν–μµλ‹λ‹¤. νλΌλ―Έν„°μ— λ€ν• μμ„Έν• μ„¤λ…μ€ μƒλµν•κ² μµλ‹λ‹¤.

- learning_rate : λ³΄ν†µ 0.01~ μ •λ„λ΅ μ„¤μ •ν•©λ‹λ‹¤. μ„Έλ¶€ μ΅°μ •μ„ μ„ν•΄μ„λ” 0.0001~μ •λ„λ΅ μ„¤μ •ν•΄λ„ λ¬΄λ°©ν•©λ‹λ‹¤.

- num_leaves : 250μ •λ„λ΅ μ„¤μ •ν•΄λ„ λ¬΄λ°©ν•©λ‹λ‹¤. 300~600 μ •λ„λ΅ μ„¤μ •ν–μµλ‹λ‹¤. 

- max_depth : -1 λ΅ μ„¤μ •ν•λ©΄ λ¬΄ν•λ€λ΅ νΈλ¦¬κ°€ κΈΈμ–΄μ§‘λ‹λ‹¤. 9~ μ •λ„λ΅ μ„¤μ •ν•λ”κ² λ¬΄λ°©ν•λ‚ μ΅°κΈ λ” λ„“μ€ λ²”μ„λ΅ μ„¤μ •ν–μµλ‹λ‹¤.

- feature_fraction, bagging_fraction  : 0κ³Ό 1 μ‚¬μ΄μ λ²”μ„λ΅ μ„¤μ •ν–μµλ‹λ‹¤.

 <br/>

λ‹¤μμ€ κ°μ²΄λ¥Ό μƒμ„±ν•©λ‹λ‹¤. 

```python
#κ°μ²΄ μƒμ„±
lgbmBO = BayesianOptimization(f = lgbm_cv, pbounds = pbounds, verbose = 2, random_state = 22 )
```

μ²«λ²μ§Έ μΈμλ” λ©μ ν•¨μ fμ΄κ³ , pboundsλ” μ…λ ¥κ°’μ νƒμƒ‰ κµ¬κ°„μ„ μλ―Έν•©λ‹λ‹¤. random seedλ” 22λ΅ μ„¤μ •ν–μµλ‹λ‹¤.

 <br/>

Bayesian Optimization κ³Όμ •μ„ μν–‰ν•κ² μµλ‹λ‹¤.

```python
# λ°λ³µμ μΌλ΅ λ² μ΄μ§€μ• μµμ ν™” μν–‰
# acq='ei'μ‚¬μ©
# xi=0.01 λ΅ explorationμ κ°•λ„λ¥Ό μ΅°κΈ λ†’μ„
lgbmBO.maximize(init_points=5, n_iter = 20, acq='ei', xi=0.01)
```

`init_points`λ” μ²μ νƒμƒ‰ νμμ…λ‹λ‹¤. pboundμ—μ„ μ„¤μ •ν• κµ¬κ°„ λ‚΄μ—μ„ `init_points` λ§νΌ μ…λ ¥κ°’μ„ μƒν”λ§ν•μ—¬ κ³„μ‚°μ΄ μ§„ν–‰λ©λ‹λ‹¤. `n_iter`μ€ μ—°μ‚° νμμ…λ‹λ‹¤. λ”°λΌμ„ μ΄ 25λ²μ„ μν–‰ν•κ² λ©λ‹λ‹¤. 

acqλ” EIλ΅ μ„¤μ •ν•κ² μµλ‹λ‹¤. xiλ” exploration-explotationμ κ°•λ„λ¥Ό μ΅°μ ν•λ” μΈμλ΅ μΌλ°μ μΌλ΅ 0.01λ΅ μ„¤μ •ν•μ—¬ explorationμ„ λ†’μ—¬μ¤λ‹λ‹¤.

μ—°μ‚° κ²°κ³Όλ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

```python

```

`target` κ°’μ€ λ©μ ν•¨μμ return κ°’μΌλ΅ auc scoreμ— ν•΄λ‹Ήν•©λ‹λ‹¤. μ•½ μµλ€ 0.7μ •λ„μ κ°’μ„ μ–»μ„ μ μμ—μµλ‹λ‹¤. 

ν¬κ² μ„±λ¥μ΄ ν–¥μƒλμ§€λ” μ•μ•μ§€λ§, μ μ€ μμ νλΌλ―Έν„° μ΅°ν•©μ„ μ¶”μ •ν•κ±°λ‚ λ¨λΈ μ•™μƒλΈ”μ„ μ§„ν–‰ν•λ‹¤λ©΄ λ”μ± μΆ‹μ€ scoreμ„ μ–»μ„ μ μμµλ‹λ‹¤.

 <br/>

κ³„μ‚°ν• νλΌλ―Έν„° κ°’μ€ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

```python
# μ°Ύμ€ νλΌλ―Έν„° κ°’ ν™•μΈ
lgbmBO.max
```

 <br/>

ν•΄λ‹Ή νλΌλ―Έν„°λ¥Ό μ μ©ν•©λ‹λ‹¤. 

```python
#νλΌλ―Έν„° μ μ©
fit_lgbm = lgbm.LGBMClassifier(learning_rate=lgbmBO.max['params']['learning_rate'],
                               num_leaves = int(round(lgbmBO.max['params']['num_leaves'])),
                               max_depth = int(round(lgbmBO.max['params']['max_depth'])),
                               min_child_weight = int(round(lgbmBO.max['params']['min_child_weight'])),
                               colsample_bytree=lgbmBO.max['params']['colsample_bytree'],
                               feature_fraction = max(min(lgbmBO.max['params']['feature_fraction'], 1), 0),
                               bagging_fraction = max(min(lgbmBO.max['params']['bagging_fraction'], 1), 0),
                               lambda_l1 = lgbmBO.max['params']['lambda_l1'],
                               lambda_l2 = lgbmBO.max['params']['lambda_l2']
                               )

model = fit_lgbm.fit(X,y)
```

λ¨λΈ μ—°μ‚°μ΄ μ™„λ£λμ—μµλ‹λ‹¤.

 <br/>

---

# 05 λ¨λΈ μ μ©

  <br/>

μ°μ„  λ¨λΈμ„ μ €μ¥ν•κ² μµλ‹λ‹¤. `joblib`μ„ μ‚¬μ©ν•λ©΄ λ¨λΈμ„ .pklλ΅ μ‰½κ² μ €μ¥ν•  μ μμµλ‹λ‹¤.

```python
import joblib
joblib.dump(model, 'model.pkl')
```

λ¨λΈμ„ μ €μ¥ν•λ©΄ μ•™μƒλΈ”μ„ μ§„ν–‰ν•κΈ° μμ›”ν•©λ‹λ‹¤. 

  <br/>

test λ°μ΄ν„° μ…‹λ„ λ¨λΈμ— μ μ©ν•μ—¬ μµμΆ… λΌλ²¨κ°’μ„ κ³„μ‚°ν•©λ‹λ‹¤.

````python
pred_y = model.predict(test)

submission['voted']=pred_y
submission.to_csv('submission.csv')
````

submission.csvλ΅ κ³„μ‚° κ²°κ³Όκ°€ μ €μ¥λμ—μµλ‹λ‹¤.

  <br/>

---

  <br/>

# **π“— μ •λ¦¬**

---

 <br/>

μ΄λ² κ²μ‹λ¬Όμ—μ„ μ†κ°ν• λ°©λ²• μ™Έμ—λ„ λ‹¤μ–‘ν• auto ml λ°©λ²•μ΄ μ΅΄μ¬ν•©λ‹λ‹¤. λ‹¤μ–‘ν• λ¨λΈμ„ μ—°μ‚°ν• ν›„ scoreκ°€ μΆ‹μ€ λ¨λΈμ„ μ•™μƒλΈ” ν•λ©΄ λ”μ± μΆ‹μ€ κ²°κ³Όλ¥Ό μ–»μ„ μ μμµλ‹λ‹¤. 

μ•„λ λ§ν¬μ—μ„ ν•΄λ‹Ή λ‚΄μ©μ λ…ΈνΈλ¶μ„ ν™•μΈν•  μ μμµλ‹λ‹¤.

- modeling_BO.ipynb [λ§ν¬](https://github.com/Jihyun22/Dacon_workspace/blob/master/psychology/modelint_BO.ipynb)

μ΄μ–΄μ§€λ” κ²μ‹λ¬Όμ—μ„λ” λ°μ΄ν„° EDAλ¥Ό μ§„ν–‰ν•κ² μµλ‹λ‹¤. 

 <br/>
