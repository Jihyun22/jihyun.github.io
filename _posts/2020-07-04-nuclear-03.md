---
title: "[원자력발전소 상태 판단 대회 리뷰] 탐색적 데이터 분석 심화(EDA)"
date: 2020-06-27
categories: [데이콘리뷰]"
toc: true
toc_sticky: true
toc_label: "목차"
tags : 원자력발전소상태판단 EDA 데이터분석 DACON ML 머신러닝 
---



# [데이콘 온라인 스터디] 3. 탐색적 데이터 분석(EDA) 심화

## 데이터를 더 압축해보자!

지난 6월부터 7월 중순까지 약 4주간에 걸쳐 원자력 상태판단대회 온라인 스터디(2기)가 진행되었습니다. 스터디에서 진행한 내용을 보다 많은 분들과 고민하고자 앞으로 4차례에 걸쳐 코드공유 게시물을 업로드할 예정입니다.

온라인 스터디와 관련한 자료는 아래 링크를 참조해 주세요.

- 데이콘 온라인 스터디 커리큘럼 [바로가기](https://www.dropbox.com/scl/fi/eaxxhf0pudm9jvckqgf4k/.papert?dl=0&rlkey=yqyrpk9eluqauoi5xjmywmp66)
- 1주차 세션 진행 자료 [바로가기](https://www.dropbox.com/scl/fi/hj22v4f47ythje8flvvm2/1.paper?dl=0&rlkey=0mrorfzb3hnvauscl459gl50z)
- 2주차 세션 진행 자료 [바로가기](https://www.dropbox.com/scl/fi/qezpyfkbj0om86afkk3bs/2.paper?dl=0&rlkey=i4nysx05x2tgy26mlkpu6cdhm)

------

세번째 주제는 <데이터를 더 압축해보자!>로, 차원분석을 통해 데이터의 경향성을 확인하고 이상치를 제거하여 데이터의 경제성을 높일 수 있는 방법에 대해 다뤄보고자 합니다.

이전 첫번째, 두번째 주제와 이어지는 게시물입니다.

- [데이콘 온라인 스터디] 1주차 - 칼럼을 2천개나 줄였습니다! 게시물 [바로가기](https://dacon.io/competitions/official/235551/codeshare/1371?page=1&dtype=recent&ptype=pub)
- [데이콘 온라인 스터디] 2주차 - 불균형한 데이터 분포, 어떻게 처리해야 할까? 게시물 [바로가기](https://dacon.io/competitions/official/235551/codeshare/1376?page=1&dtype=recent&ptype=pub)

*(이하 내용은 원자력 온라인 스터디 1, 2기에서 다루었던 내용이 포함되어 있습니다.)*

----


### 5. 차원축소

첫번째 게시글을 통해 칼럼을 약 2000여개를 drop 하였지만, 그럼에도 약 3000개의 칼럼이 남아 있습니다. 데이터의 경향성과 압축 가능성을 확인하기 위해 차원축소를 진행하였습니다.

우선 데이터를 로드하겠습니다. 방식은 이전 게시물과 동일합니다.


```python
import multiprocessing 
from multiprocessing import Pool 
from functools import partial 
from data_loader_v2 import data_loader_v2
import os 
import pandas as pd
import numpy as np
import joblib
```


```python
train_folder = 'train/'
train_list = os.listdir(train_folder)
train_label_path = 'train_label.csv'
train_label = pd.read_csv(train_label_path, index_col=0)
```


```python
def data_loader_all_v2(func, files, folder='', train_label=None, event_time=15, nrows=75):   
    func_fixed = partial(func, folder=folder, train_label=train_label, event_time=event_time, nrows=nrows)  
    if __name__ == '__main__':
        pool = Pool(processes=multiprocessing.cpu_count()) 
        df_list = list(pool.imap(func_fixed, files)) 
        pool.close() 
        pool.join() 
    combined_df = pd.concat(df_list)
    return combined_df
```


```python
# event_time=15, nrows=75 설정
train = data_loader_all_v2(data_loader_v2, train_list, folder=train_folder, train_label=train_label, event_time=15, nrows=75)

# 의미 없는 칼럼 드랍(첫번째 게시물 참조)
train = train.loc[:,train.std()!=0]

X_train = train.drop(['label'], axis=1)
y_train = train['label']
```

차원축소는 비지도 차원축소 방식인 PCA를 적용하였습니다.


```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 스케일링
scaler = StandardScaler()
df_scaled=scaler.fit_transform(X_train)

# pca 차원축소
pca=PCA(n_components=3)
df_pca = pca.fit_transform(df_scaled)
```

결과를 3차원 그래프로 시각화하여 확인해보겠습니다.


```python
#시각화
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.set_zlabel("x_composite_3")

ax.scatter(df_pca[:, 0], df_pca[:, 1], zs=df_pca[:, 2], s=4, lw=1, label="inliers",c="green")
ax.legend()
plt.show()
```


![3d 데이터 분포도](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/2020-07-04-nuclear-03/3d%20%EB%B6%84%ED%8F%AC%EB%8F%84.png?raw=true)


어느 정도의 경향성을 가지지만, 일부 이상치로 볼 수 있는 데이터도 존재하는 것 처럼 보입니다. 압축 가능성을 확인하기 위해 단순 rf모델로 성능 비교를 진행했습니다.


```python
# 차원축소 성능평가
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rcf=RandomForestClassifier(n_estimators=300, random_state=156)
scores_pca = cross_val_score(rcf,df_pca, y_train, scoring='accuracy')

print('CV3인 경우 PCA변환된 개별 fold 세트별 정확도 : ', scores_pca)
print('PCA 변환 데이터 세트 평균 정확도 : {0:.4f}'.format(np.mean(scores_pca)))
```

    CV3인 경우 PCA변환된 개별 fold 세트별 정확도 :  [0.05309855 0.10810811 0.13772864 0.13745564 0.13390663]
    PCA 변환 데이터 세트 평균 정확도 : 0.1141




그러나 실제 rf 결과는 예상과 많이 달랐습니다. 평균 정확도 약 0.1141 정도로 상당히 성능이 저하된 것을 확인할 수 있습니다. 스케일링의 문제인가 싶어 standard 외 robust, minmax 등을 적용하여 비교하였으나, 성능에 큰 변화는 없었습니다.

따라서, 차원 축소를 적용하는 것은 적절하지 않다고 판단됩니다.

----

### 6. 이상치탐색

지금까지 칼럼 축소를 위주로 데이터 EDA를 진행하였는데, 이제 train 데이터셋의 row 데이터를 좀 더 살펴보겠습니다. 우리가 사용하고 있는 데이터 로드 모듈, data_loader_v2.py를 보면, 불러온 train 데이터의 row는 file id임을 알 수 있습니다.

따라서 row를 기준으로 이상치 탐색을 진행하면, 비교적 중요도가 낮은 file id를 제거할 수 있습니다. lsolation forest를 적용하여 이상치를 탐색을 진행했습니다. contamination변수를 0.01로 설정하여 전체 데이터 중 1%를 이상치로 간주하고 lf모델을 통해 이상치를 탐색한 결과입니다.


```python
from sklearn.ensemble import IsolationForest
cif=IsolationForest(n_estimators=50, max_samples=50, contamination=float(0.01), max_features=1.0, bootstrap=False, n_jobs=-1, random_state=None, verbose=0,behaviour="new")
cif.fit(X_train)

pred = cif.predict(X_train)
X_train['out']=pred
```



```python
# 1이면 정상, -1이면 이상치 포함된 index
tmp=X_train.loc[X_train['out']==-1]
out_index=list(tmp.index)
```


```python
# 이상치 row
tmp
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V0000</th>
      <th>V0001</th>
      <th>V0002</th>
      <th>V0003</th>
      <th>V0004</th>
      <th>V0005</th>
      <th>V0006</th>
      <th>V0007</th>
      <th>V0008</th>
      <th>V0009</th>
      <th>...</th>
      <th>V5085</th>
      <th>V5086</th>
      <th>V5087</th>
      <th>V5088</th>
      <th>V5089</th>
      <th>V5090</th>
      <th>V5115</th>
      <th>V5118</th>
      <th>V5119</th>
      <th>out</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>103</td>
      <td>23.559128</td>
      <td>4.274427</td>
      <td>4.250021</td>
      <td>3.751442</td>
      <td>3.759234</td>
      <td>-8.291305</td>
      <td>34.442123</td>
      <td>2.650298e+02</td>
      <td>0.0</td>
      <td>261.567632</td>
      <td>...</td>
      <td>110.859694</td>
      <td>-0.300430</td>
      <td>-0.234850</td>
      <td>-0.148066</td>
      <td>-0.148180</td>
      <td>43.194587</td>
      <td>0.1000</td>
      <td>70.678227</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>103</td>
      <td>23.544267</td>
      <td>4.354535</td>
      <td>4.258811</td>
      <td>3.740691</td>
      <td>3.782363</td>
      <td>2.535554</td>
      <td>6.121905</td>
      <td>2.650298e+02</td>
      <td>0.0</td>
      <td>261.565717</td>
      <td>...</td>
      <td>110.921065</td>
      <td>-0.301900</td>
      <td>-0.229540</td>
      <td>-0.225293</td>
      <td>-0.141646</td>
      <td>43.199915</td>
      <td>0.1000</td>
      <td>70.678226</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>103</td>
      <td>23.538900</td>
      <td>4.288717</td>
      <td>4.233603</td>
      <td>3.788568</td>
      <td>3.805828</td>
      <td>2.925952</td>
      <td>-7.025525</td>
      <td>2.650298e+02</td>
      <td>0.0</td>
      <td>261.564724</td>
      <td>...</td>
      <td>110.941848</td>
      <td>-0.303200</td>
      <td>-0.257178</td>
      <td>-0.270651</td>
      <td>-0.188651</td>
      <td>43.197637</td>
      <td>0.1000</td>
      <td>70.678220</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>103</td>
      <td>23.561275</td>
      <td>4.308862</td>
      <td>4.239103</td>
      <td>3.765229</td>
      <td>3.797751</td>
      <td>13.439356</td>
      <td>-27.703969</td>
      <td>2.650298e+02</td>
      <td>0.0</td>
      <td>261.568362</td>
      <td>...</td>
      <td>110.876239</td>
      <td>-0.292275</td>
      <td>-0.222797</td>
      <td>-0.185541</td>
      <td>-0.161072</td>
      <td>43.191146</td>
      <td>0.1000</td>
      <td>70.678230</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>103</td>
      <td>23.546555</td>
      <td>4.228655</td>
      <td>4.257381</td>
      <td>3.758402</td>
      <td>3.783031</td>
      <td>-15.119784</td>
      <td>-10.500483</td>
      <td>2.650298e+02</td>
      <td>0.0</td>
      <td>261.567716</td>
      <td>...</td>
      <td>110.877679</td>
      <td>-0.288895</td>
      <td>-0.245284</td>
      <td>-0.236787</td>
      <td>-0.170444</td>
      <td>43.205434</td>
      <td>0.1000</td>
      <td>70.678220</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>176</td>
      <td>24.044489</td>
      <td>4.286894</td>
      <td>4.213520</td>
      <td>3.786237</td>
      <td>3.782339</td>
      <td>-1.313453</td>
      <td>-21.468657</td>
      <td>2.651105e+02</td>
      <td>0.0</td>
      <td>261.569403</td>
      <td>...</td>
      <td>110.912965</td>
      <td>-0.272413</td>
      <td>-0.246441</td>
      <td>-0.198147</td>
      <td>-0.145400</td>
      <td>43.193136</td>
      <td>0.1000</td>
      <td>70.678221</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>176</td>
      <td>24.059724</td>
      <td>4.353295</td>
      <td>4.240205</td>
      <td>3.774026</td>
      <td>3.795141</td>
      <td>8.448634</td>
      <td>-20.689173</td>
      <td>2.651098e+02</td>
      <td>0.0</td>
      <td>261.569243</td>
      <td>...</td>
      <td>110.848742</td>
      <td>-0.293415</td>
      <td>-0.233067</td>
      <td>-0.183883</td>
      <td>-0.140670</td>
      <td>43.203261</td>
      <td>0.1000</td>
      <td>70.678225</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>176</td>
      <td>24.046723</td>
      <td>4.269111</td>
      <td>4.266959</td>
      <td>3.749095</td>
      <td>3.772972</td>
      <td>8.307168</td>
      <td>-17.935429</td>
      <td>2.651105e+02</td>
      <td>0.0</td>
      <td>261.570471</td>
      <td>...</td>
      <td>110.971647</td>
      <td>-0.307250</td>
      <td>-0.241933</td>
      <td>-0.174509</td>
      <td>-0.140263</td>
      <td>43.189406</td>
      <td>0.1000</td>
      <td>70.678208</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>176</td>
      <td>24.054898</td>
      <td>4.275377</td>
      <td>4.252281</td>
      <td>3.760244</td>
      <td>3.782365</td>
      <td>19.138578</td>
      <td>-37.828581</td>
      <td>2.651105e+02</td>
      <td>0.0</td>
      <td>261.569444</td>
      <td>...</td>
      <td>110.931214</td>
      <td>-0.282216</td>
      <td>-0.227733</td>
      <td>-0.212222</td>
      <td>-0.163397</td>
      <td>43.196402</td>
      <td>0.1000</td>
      <td>70.678223</td>
      <td>8.82437</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>290</td>
      <td>30.460602</td>
      <td>7.366373</td>
      <td>7.372645</td>
      <td>7.308110</td>
      <td>7.372311</td>
      <td>278.581987</td>
      <td>247.749568</td>
      <td>2.854127e-19</td>
      <td>0.0</td>
      <td>265.488183</td>
      <td>...</td>
      <td>110.829413</td>
      <td>-0.288924</td>
      <td>-0.247073</td>
      <td>-0.146469</td>
      <td>-0.163237</td>
      <td>43.204391</td>
      <td>58.9863</td>
      <td>-0.000011</td>
      <td>5.79981</td>
      <td>-1</td>
    </tr>
  </tbody>
</table>
<p>367 rows × 3456 columns</p>

</div>



총 367개의 칼럼이 탐색되었습니다. 중복값을 제거하지 않은 out_index를 보면 이상치로 판단된 file id는 대부분 103, 176 위주로, 해당 file id만을 drop하기로 결정했습니다. 결과는 다음과 같습니다.


```python
print(out_index)
```

    [103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 290]



```python
# id 103, 176만 삭제
out_index.remove(290)
X_train = X_train.drop(['out'], axis=1)

X_train = X_train.drop(out_index)
y_train = y_train.drop(out_index)
```


```python
X_train.shape, y_train.shape
```

((36260, 3455), (36260,))




```python
# 제거한 row
X_train
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V0000</th>
      <th>V0001</th>
      <th>V0002</th>
      <th>V0003</th>
      <th>V0004</th>
      <th>V0005</th>
      <th>V0006</th>
      <th>V0007</th>
      <th>V0008</th>
      <th>V0009</th>
      <th>...</th>
      <th>V5084</th>
      <th>V5085</th>
      <th>V5086</th>
      <th>V5087</th>
      <th>V5088</th>
      <th>V5089</th>
      <th>V5090</th>
      <th>V5115</th>
      <th>V5118</th>
      <th>V5119</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>30.469574</td>
      <td>8.722739</td>
      <td>8.686953</td>
      <td>8.677701</td>
      <td>8.696935</td>
      <td>215.779134</td>
      <td>148.857105</td>
      <td>-8.951266e-20</td>
      <td>0.0</td>
      <td>-0.000694</td>
      <td>...</td>
      <td>110.922023</td>
      <td>110.958197</td>
      <td>-0.298096</td>
      <td>-0.234462</td>
      <td>-0.241420</td>
      <td>-0.164439</td>
      <td>43.197957</td>
      <td>60.0</td>
      <td>-2.988467e-06</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>0</td>
      <td>30.471422</td>
      <td>8.843733</td>
      <td>8.724614</td>
      <td>8.736648</td>
      <td>8.724141</td>
      <td>189.935527</td>
      <td>186.819255</td>
      <td>5.018471e-19</td>
      <td>0.0</td>
      <td>0.001233</td>
      <td>...</td>
      <td>110.918918</td>
      <td>110.930774</td>
      <td>-0.311288</td>
      <td>-0.229316</td>
      <td>-0.230339</td>
      <td>-0.174198</td>
      <td>43.194910</td>
      <td>60.0</td>
      <td>1.136327e-05</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>0</td>
      <td>30.465795</td>
      <td>8.639923</td>
      <td>8.693430</td>
      <td>8.706842</td>
      <td>8.698667</td>
      <td>167.172015</td>
      <td>227.642581</td>
      <td>5.601811e-19</td>
      <td>0.0</td>
      <td>0.000029</td>
      <td>...</td>
      <td>110.923194</td>
      <td>110.877289</td>
      <td>-0.275290</td>
      <td>-0.247143</td>
      <td>-0.175712</td>
      <td>-0.179520</td>
      <td>43.206854</td>
      <td>60.0</td>
      <td>1.157871e-06</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>0</td>
      <td>30.451257</td>
      <td>8.643156</td>
      <td>8.721100</td>
      <td>8.677412</td>
      <td>8.697360</td>
      <td>190.645984</td>
      <td>181.005102</td>
      <td>4.328276e-19</td>
      <td>0.0</td>
      <td>-0.001779</td>
      <td>...</td>
      <td>110.822725</td>
      <td>110.848307</td>
      <td>-0.286780</td>
      <td>-0.240937</td>
      <td>-0.212229</td>
      <td>-0.157415</td>
      <td>43.198779</td>
      <td>60.0</td>
      <td>4.229167e-06</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>0</td>
      <td>30.469449</td>
      <td>8.786702</td>
      <td>8.718487</td>
      <td>8.632532</td>
      <td>8.686938</td>
      <td>208.447021</td>
      <td>202.666961</td>
      <td>3.835715e-19</td>
      <td>0.0</td>
      <td>-0.000019</td>
      <td>...</td>
      <td>110.810634</td>
      <td>110.926518</td>
      <td>-0.301789</td>
      <td>-0.237373</td>
      <td>-0.201726</td>
      <td>-0.188566</td>
      <td>43.204087</td>
      <td>60.0</td>
      <td>1.503048e-06</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>822</td>
      <td>30.466798</td>
      <td>8.781396</td>
      <td>8.656021</td>
      <td>8.741145</td>
      <td>8.678219</td>
      <td>183.032479</td>
      <td>198.732448</td>
      <td>5.160292e-19</td>
      <td>0.0</td>
      <td>-0.002553</td>
      <td>...</td>
      <td>110.878279</td>
      <td>110.853071</td>
      <td>-0.292044</td>
      <td>-0.230561</td>
      <td>-0.188257</td>
      <td>-0.138816</td>
      <td>43.196687</td>
      <td>60.0</td>
      <td>4.202532e-07</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>822</td>
      <td>30.487399</td>
      <td>8.813155</td>
      <td>8.718072</td>
      <td>8.723063</td>
      <td>8.724389</td>
      <td>209.083991</td>
      <td>153.851021</td>
      <td>-6.571058e-19</td>
      <td>0.0</td>
      <td>0.000800</td>
      <td>...</td>
      <td>110.852287</td>
      <td>110.798022</td>
      <td>-0.281708</td>
      <td>-0.233645</td>
      <td>-0.252706</td>
      <td>-0.165436</td>
      <td>43.201587</td>
      <td>60.0</td>
      <td>3.512496e-07</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>822</td>
      <td>30.467739</td>
      <td>8.752970</td>
      <td>8.674999</td>
      <td>8.683145</td>
      <td>8.694938</td>
      <td>194.415606</td>
      <td>250.116154</td>
      <td>4.640560e-20</td>
      <td>0.0</td>
      <td>-0.001554</td>
      <td>...</td>
      <td>111.049257</td>
      <td>110.944531</td>
      <td>-0.293674</td>
      <td>-0.216244</td>
      <td>-0.205386</td>
      <td>-0.175538</td>
      <td>43.207345</td>
      <td>60.0</td>
      <td>-6.612851e-06</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>822</td>
      <td>30.451295</td>
      <td>8.725999</td>
      <td>8.702534</td>
      <td>8.673263</td>
      <td>8.707673</td>
      <td>216.079464</td>
      <td>197.389922</td>
      <td>-1.351444e-19</td>
      <td>0.0</td>
      <td>-0.000090</td>
      <td>...</td>
      <td>111.054845</td>
      <td>111.045120</td>
      <td>-0.287287</td>
      <td>-0.242544</td>
      <td>-0.238269</td>
      <td>-0.135583</td>
      <td>43.199317</td>
      <td>60.0</td>
      <td>-3.596244e-05</td>
      <td>85.4</td>
    </tr>
    <tr>
      <td>822</td>
      <td>30.464721</td>
      <td>8.819425</td>
      <td>8.706992</td>
      <td>8.655707</td>
      <td>8.720076</td>
      <td>181.483629</td>
      <td>187.133303</td>
      <td>9.229799e-20</td>
      <td>0.0</td>
      <td>-0.001376</td>
      <td>...</td>
      <td>110.820025</td>
      <td>110.830310</td>
      <td>-0.295821</td>
      <td>-0.231805</td>
      <td>-0.202189</td>
      <td>-0.132303</td>
      <td>43.205785</td>
      <td>60.0</td>
      <td>-2.213414e-05</td>
      <td>85.4</td>
    </tr>
  </tbody>
</table>
<p>36260 rows × 3455 columns</p>

</div>



이상치 제거 결과의 성능 평가를 위해 단순 rf 모델을 사용하여 원본 데이터 결과와 비교해보았습니다. 원본 데이터의 rf 정확도 점수는 첫번째 게시물 또는 아래 결과 화면 사진을 참고해주세요.

![rf결과](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/2020-07-04-nuclear-03/%EC%B0%B8%EA%B3%A0%EC%9E%90%EB%A3%8C%202.jpg?raw=true)


```python
#file id제거 성능평가
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rcf=RandomForestClassifier(n_estimators=300)
scores = cross_val_score(rcf, X_train, y_train, scoring='accuracy', cv=3)
print('CV3인 경우 개별 fold 세트별 정확도 : ', scores)
print('평균 정확도 : {0:.4f}'.format(np.mean(scores)))
```

    CV3인 경우 개별 fold 세트별 정확도 :  [0.76302639 0.84346819 0.85268906]
    평균 정확도 : 0.8197


결과는 약 정확도 0.82정도로 약간의 성능 개선 효과를 볼 수 있었습니다.

덧붙이자면 file id 103은 라벨 54에 속하고, file id 176은 라벨 53에 속합니다. 또한 라벨 54는 12개, 53은 7개의 file이 속합니다. 최대값 24(라벨 110)에 비하면 적은 수치이기에 적은 file이 할당된 라벨의 file을 제거하는 것이 적절한지 많은 고민을 하게 되었습니다.

lf모델의 contamination변수를 0.002~0.01로 변화를 주어도 결과는 동일하였습니다. 여러 lf 모델을 통해 동일한 결과가 이상치로 판단되었다는 점에서 해당 id를 제거하는 방향으로 결정하였고, 위 결과처럼 단순 rf모델로 성능 개선 효과를 확인할 수 있었습니다.

지금까지 데이터 전처리에 대해 살펴보았습니다. 다음 게시물에서는 모델링을 다루겠습니다.



---



---

<center> <BIG>[데이콘리뷰] 관련 목록 바로가기 </BIG> </center>

---



