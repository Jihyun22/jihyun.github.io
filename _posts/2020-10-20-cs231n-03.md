---
title: "[CS231n] Lecture 3 - Loss Fuctions and Optimization"
date: 2020-10-20
categories: dl
toc: true
toc_sticky: true
toc_label: "목차"
tags : data dl datastudy cs231n
related: true
header:
  teaser: "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/cs231n.png?raw=true"
---



<img src= "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/cs231n.png?raw=true">

스탠포드 대학의 CS231n 강의노트입니다. 

CS231n강의는 1~16강으로 구성되어 있으며 이번 포스트는 **Lecture 3 - Loss Fuctions and Optimization** 에 대해 다루겠습니다.

*가짜연구소*의 *딥러닝 이론 스터디* 활동으로 작성되었습니다.

- youtube 강의 영상 바로가기 [링크](https://youtu.be/vT1JzLTH4G4?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk)
- 가짜연구소 사이트 바로가기 [링크](https://pseudo-lab.com/)

<center><p>
  <img src="https://visitor-badge.laobi.icu/badge?page_id=jihyun22.github.io/dl/cs231n-02/" alt="visitor"/>
    </p></center>




---

<br/>



# 🚀 **개요** 

---

<br/>

3강에서는 2강에 이어 최적의 W(가중치)를 구하는 방법에 대해 소개합니다.

W을 평가하는 Loss Fuction(손실함수)를 사용하여 최적화 과정에 대해 알아보겠습니다.

<br/>



---

## Multi class SVM Loss

---

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/12.png?raw=true)

실제 손실함수의 계산 과정입니다. 
$$
L_{i} = \displaystyle \sum _{j \ne  y_{i}} { max( 0, S_{j} - S_{y_{i}} + 1 )}
$$
위 식에서

- Sj = 예측된 평가 score
- Syi = 실제 정답 클래스의 score

입니다.

이때 주목해야 하는 점은 스코어의 값 보다 **스코어 간 차이** 입니다.

<br/>

`frog` 클래스를 예시로 살펴보겠습니다. 

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/16.png?raw=true)

<br/>

`frog` 클래스 계산 순서를 자세히 살펴보겠습니다.

우선 Syi의 값은 -3.1로 고정값을 가지며, `cat` 클래스와 max연산을 진행하면 max(0, 2.2 -+3.1 +1) = 6.3 값이 연산됩니다. 마찬가지로 `car` 클래스와 비교하면 6.6의 값을 얻게 되며, 최종 Li의 값은 12.9 (6.6+6.3) 입니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/24.png?raw=true)

</br>

코드로 살펴보겠습니다.

Loss는 0~ 값을 갖게 되며 자기 클래스를 제외한 다른 클래스에 대한 손실함수를 계산한 후 스코어들이 거의 0에 가깝고 값이 비슷하다면 Loss는 클래스의 개수인 n-1 의 값을 얻게 됩니다. 

이때, Loss의 최솟값인 0이 되게 하는 W는 유일하지 않습니다. W이 2W, 3W... 이 되어도 Loss값은 변함없기 떄문입니다.

따라서 Loss가 단순히 최솟값, 0이 되게 하는 W를 찾는 것은 train 데이터에 오버피팅 되는 결과를 초래할 수 있습니다. 

</br>

---

## Regularization 

---

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/33.png?raw=true)

<br/>

일반적으로 train 데이터에 과적합(overfitting)되는 결과를 막기 위해 정규화 항을 추가하여 사용합니다. 

정규화 항을 통해 더욱 복잡한 분류기를 간단하게 제어할 수 있습니다. 즉, test 데이터에도 균일한 값이 산출될 수 있도록 합니다.

그 중 가장 많이 사용되는 정규화 항인 L2에 대해 설명하겠습니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/34.png?raw=true)

<br/>

L2는 가중치 값이 비교적 균일하게 분포되어 있을 때 덜 복잡한 것으로 간주합니다.

이에 비해 L1은 가중치가 0이 아닌 요소들의 개수가 적을수록 덜 복잡한 것으로 판단합니다.

예시로 살펴보겠습니다.

가중치 조합 w1, w2에 대해

- w1 = [0,0,0,1]
- w2 = [0.25, 0.25, 0.25, 0.25]

가 있다고 가정합니다. 보다 쉽게 비교하기 위해 극단적인 값을 선정했습니다.

정규화 항 L2를 사용하는 경우, w의 요소가 한쪽에 치중되어 있으면 복잡하다고 간주하기 때문에 균일하게 분포되어 있는 w2값이 선택됩니다. 

이에 비해 L1을 사용하는 경우, 0이 아닌 요소가 많은 w2가 더욱 복잡한 것으로 간주되어 w1이 선택됩니다.

<br/>

---

## Softmax

---

<br/>

지금까지는 score자체에 의미 보다 score의 차이에 대해 살펴보았습니다.

다음은 score에 의미를 부여하는 softmax에 대해 알아보겠습니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/40.png?raw=true)

<br/>

softmax는 딥러닝에서 자주 사용되는 개념입니다.

SVM은 score에 대한 해석을 고려하지 않고 정답인 클래스가 다른 클래스들 보다 더욱 높은 score을 얻기 위한 비교 연산을 수행했습니다.

softmax는 이와 달리 score에 의미를 부여하여 일련의 연산을 통해 확률 분포를 얻어 정답 클래스일 확률을 바로 산출합니다. 

이때 Loss 값은 -log연산을 사용합니다. 

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/46.png?raw=true)

<br/>

연산 과정은 주어진 스코어(정규화 되지 않은 log probabilities)에 대해 **exp 연산을 거쳐** 점수를 반환합니다.

산출된 값을 -log 연산을 취해 최종 probabilities을 계산합니다.

exp 연산을 거치는 계산이 softmax 연산에 해당되며, 일련의 변환 과정을 softmax, 또는 multinomial logistic regression이라고 합니다.

softmax의 값은 0~의 값을 가집니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/49.png?raw=true)

<br/>

SVM 과 Softmax의 가장 큰 차이는 성능 개선 측면에 있습니다.

SVM 의 경우 일정 차이, 즉 margin 을 넘기만 하면 더이상 성능 개선이 이루어지지 않지만, 

정답 클래스의 -log 점수를 계산하는 Softmax의 경우 최대한 확률을 높이기 위한 연산이 반복되며 성능이 향상됩니다.

<br/>

---

## Optimization

---

<br/>

그렇다면 Loss를 최소화하는 최종 W는 어떻게 찾을 수 있을까요?

이러한 고민을 해결하기 위해 도입된 방법이 최적화(Optimization)입니다.

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/60.png?raw=true)

<br/>

그 중, Slope(기울기)를 이용한 최적화 방법을 살펴보겠습니다.

이때 기울기는 어떤 함수에 대한 미분값을 의미합니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/62.png?raw=true)

<br/>

`gradient` 는 벡터 x의 편도함수의 집합입니다. 즉, 해당 벡터의 방향으로 이동했을 때 Loss의 변화값을 나타내는 값 입니다.

하지만, 일일히 계산하는 것은 상당히 비경제적이므로 실제 `gradient` 를 계산하기 위해서는 보다 분석적인 방법을 이용합니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/76.png?raw=true)

<br/>

가장 많이 사용되는 gradient 계산 방법은 SGD입니다.

gradient 와 loss를 계산하기 보다 Minibatch라는 샘플 집합을 나눠 학습을 진행하는 방법으로, Minibatch의 크기는 2의 승수로 정해집니다.

Minibatch의 계산 결과를 바탕으로 전체 loss, gradient의 추정치를 계산할 수 있습니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/79.png?raw=true)

<br/>

우선 이미지에 대해 특징을 특징 벡터로 변환합니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-3/80.png?raw=true)

<br/>

이후 데이터의 좌표계를 조정하여 벡터를 선형으로 분리합니다. 이러한 과정을 통해 어떠한 특징 벡터가 존재하는지 확인할 수 있고, 어떠한 변환 과정을 적용해야 적절한지 판단할 수 있습니다.

예를 들어, 어떠한 개구리 사진의 특징 벡터가 색깔인 경우 color histogram을 구성하여 적절한 변환을 진행할 수 있습니다.

<br/>

# **📗 정리**

---

 <br/>

이전의 분류기와 CNN의 가장 큰 차이점은 이미 구성된 특징을 사용하는 것이 아닌 데이터로부터 직접 특징을 학습한다는 점 입니다.

이러한 방식을 통해 선형 분류기와 더불어 가중치 셋 전체를 추가하여 학습을 진행할 수 있습니다.

다음은 Neural Networks 에 대한 내용을 살펴보겠습니다.

 <br/>


