---
title: "[CS231n] Lecture 6 - Training Neural Networks, Part I"
date: 2020-11-14
categories: dl
toc: true
toc_sticky: true
toc_label: "목차"
tags : data dl datastudy cs231n
related: true
header:
  teaser: "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/cs231n.png?raw=true"
---



<img src= "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/cs231n.png?raw=true">

스탠포드 대학의 CS231n 강의노트입니다. 

CS231n강의는 1~16강으로 구성되어 있으며 이번 포스트는 **Lecture 6 - Training Neural Networks, Part I"** 에 대해 다루겠습니다.

*가짜연구소*의 *딥러닝 이론 스터디* 활동으로 작성되었습니다.

- youtube 강의 영상 바로가기 [링크](https://youtu.be/vT1JzLTH4G4?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk)
- 가짜연구소 사이트 바로가기 [링크](https://pseudo-lab.com/)

<center><p>
  <img src="https://visitor-badge.laobi.icu/badge?page_id=jihyun22.github.io/dl/cs231n-06/" alt="visitor"/>
    </p></center>






---

<br/>



# 🚀 **개요** 

---

<br/>

지난 5강에서는 computational graph, f=Wx+regularization 와 CNN의 activation map 등을 공부했습니다. 이번 6강에서는 Training Neural Networks 와 관련된 내용을 다루겠습니다.

<br/>



---

## 활성화함수

---

뉴런 네트워크는 이번 6장과 다음 7장까지 2번에 걸쳐서 이어집니다. 

우선 활성화함수, activation fuction 에 대해 설명하겠습니다. 

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-6/15.jpg?raw=true)

활성화함수는 f=Wx에 대해 input이 들어오면 다음 노드로 보낼 때 어떠한 형식으로 값을 도출하는지 결정합니다. 따라서 no-linear 한 함수를 사용해야 합니다.

즉, input의 형식은 linear 하지만 출력값은 non linear 한 값이 나오게 됩니다.

선형 값을 사용하게 되면 여러 레이어가 쌓인 복잡한 구조이더라도, 1개의 레이어와 동일한 효과를 내기 때문에 비선형 값을 사용합니다.

- y(x) = h(h(h(x)))이란 식이
- y = C^3 선형으로 표현되는 결과가 초래됨



## CNN

---

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/26.jpg?raw=true)

<br/>

CNN구조를 이해하기 위해서는 Fully Connected Layer 을 이해해야 합니다.

이 구조는 어떤 벡터를 가지고 연산을 할 때 layer 간 모든 요소의 백터가 서로 연결된 구조를 의미합니다. 이때 activation은 해당 layer의 출력입니다.

<br/>



![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/29.jpg?raw=true)

<br/>

기존의 FC Layer가 입력 이미지를 1차원 벡터로 변형시켜 연산을 진행했다면, 이제는 기존의 이미지 구조를 그대로 유지하여 input 입력 데이터로 사용합니다.

이후 위 슬라이드에서 filter 라는 가중치 벡터를 통과해서 내적 연산을 수행합니다. 

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/31.jpg?raw=true)

<br/>

이때 원본 데이터의 깊이, 3은 전체를 취하지만, 32* 32 중 filter 의 크기인 5*5만 취해서 해당 이미지의 픽셀을 곱해줍니다. 

물론, 각 벡터 간 convolution 을 하거나, 이미지를 1차원으로 펼쳐 내적 연산을 수행하는 것과 동일한 연산입니다.

결과값은 필터당 1개로, 여러개의 필터를 사용하면 여러 activation map을 얻을 수 있습니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/36.jpg?raw=true)

<br/>

각 레이어의 출력은 다음 layer의 입력이 됩니다.

이때 pooling은 activation maps의 사이즈를 줄이는 역할을 수행합니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/37.jpg?raw=true)

필터를 많이 통과할수록, 즉 layer의 계층이 높아질 수록 더 많은 특징을 담아낼 수 있습니다.

<br/>

---

## CNN의 연산 과정

---

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/47.jpg?raw=true)

<br/>

다음은 stride의 개념을 알아보겠습니다.

stride는 한 칸씩 움직이는 정도를 의미하며, 이때 stride가 input 이미지에 맞지 않으면 불균형한 결과를 부를 수도 있습니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/52.jpg?raw=true)

<br/>

따라서 해당 필터를 통과하면 얻을 수 있는 output 사이즈는 (input size-filter) / stride +1 입니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/53.jpg?raw=true)

<br/>

추가로, Zero-padding은 가장자리의 필터 연산을 수행하여 입력의 사이즈를 유지하는 역할을 수행합니다. 위와 같은 연산을 수행하면 새 출력이 7*7이 되며 출력의 차원이 입력의 차원과 같아지게 됩니다.

이때, Zero-padding을 하지 않으면, Layer가 쌓이면서 output 사이즈가 급격히 줄어들게 돕니다. 이러한 결과는 activation map이 작아지기 때문에 바람직한 결과가 아닙니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-5/60.jpg?raw=true)

실제 계산을 진행하면 위와 같이 계산되며 최종, 파라미터의 개수는 760입니다.



<br/>

---



# **📗 정리**

---

 <br/>

뉴런 네트워크의 활성화함수와 전처리 방법에 대해 다루었습니다.

다음 7장에서는 뉴런 네트워크의 2번째 내용을 정리하도록 하겠습니다.

 <br/>


