---
title: "[ë°ì´ì½˜ ì‹¬ë¦¬ì„±í–¥ ì˜ˆì¸¡ ëŒ€íšŒ] AUTO ML - ë² ì´ì§€ì•ˆ ìµœì í™” (Bayesian Optimization)"
date: 2020-10-06
categories: ë°ì´ì½˜ë¦¬ë·°
toc: true
toc_sticky: true
toc_label: "ëª©ì°¨"
tags : data dacon contest
related: true
header:
  teaser: "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/psychology.png?raw=true"
---



<img src= "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/psychology.png?raw=true">

ì›”ê°„ë°ì´ì½˜ 8 ì‹¬ë¦¬ ì„±í–¥ ì˜ˆì¸¡ ê²½ì§„ ëŒ€íšŒ ë¦¬ë·° 2í¸ì…ë‹ˆë‹¤. 

- ë°ì´ì½˜ ëŒ€íšŒ ì•ˆë‚´ ë°”ë¡œê°€ê¸° [ë§í¬](https://dacon.io/competitions/official/235647/overview/)
- [ë°ì´ì½˜ ì‹¬ë¦¬ì„±í–¥ ì˜ˆì¸¡ ëŒ€íšŒ] ë¦¬ë·° 1í¸ ë°”ë¡œê°€ê¸° [ë§í¬](jihyun22.github.io/ë°ì´ì½˜ë¦¬ë·°/psychology-01/)

<center><p>
  <img src="https://visitor-badge.laobi.icu/badge?page_id=jihyun22.github.io/ë°ì´ì½˜ë¦¬ë·°/psychology-02/" alt="visitor"/>
    </p></center>


---

<br/>



# ğŸš€ **ê°œìš”** 

---

<br/>

ì´ë²ˆ ì›”ê°„ ë°ì´ì½˜ì˜ ì‹¬ë¦¬ ì„±í–¥ ì˜ˆì¸¡ ê²½ì§„ëŒ€íšŒëŠ” ë§ˆí‚¤ì•„ë°¸ë¦¬ì¦˜ ì‹¬ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì°¸ê°€ìì˜ êµ­ê°€ ì„ ê±° íˆ¬í‘œ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëŒ€íšŒì…ë‹ˆë‹¤.

1í¸ [ë°ì´ì½˜ ì‹¬ë¦¬ì„±í–¥ ì˜ˆì¸¡ ëŒ€íšŒ] ë¬¸ìì—´ì´ í¬í•¨ëœ ë°ì´í„° ì²˜ë¦¬í•˜ê¸° [ë§í¬]((https://jihyun22.github.io/%EB%8D%B0%EC%9D%B4%EC%BD%98%EB%A6%AC%EB%B7%B0/psychology-01/))ì—ì„œëŠ” Label encoding ê³¼ ê´€ë ¨í•˜ì—¬ `get_dummies()` ë¥¼ ì‚¬ìš©í•œ ë¬¸ìì—´ì´ í¬í•¨ëœ ë³€ìˆ˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. 

- [ë°ì´ì½˜ ì‹¬ë¦¬ì„±í–¥ ì˜ˆì¸¡ ëŒ€íšŒ] ë¦¬ë·° 1í¸ ë°”ë¡œê°€ê¸° [ë§í¬](jihyun22.github.io/ë°ì´ì½˜ë¦¬ë·°/psychology-01/)

<br/>

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Auto ML ë°©ë²• ì¤‘ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì— ëŒ€í•´ ë‹¤ë£¨ë©°,  Bayesian Optimization(ë² ì´ì§€ì•ˆ ìµœì í™”) ì ìš© ë°©ë²•ì„ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤. Bayesian Optimizationì˜ ìì„¸í•œ ì„¤ëª…ì€ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

- [AUTO ML] ë² ì´ì§€ì•ˆ ìµœì í™” (Bayesian Optimization) [ë§í¬](jihyun22.github.io/automl/Bayesian Optimization/)




---

<br/>



# **01~2 data_encoding.ipynb ì°¸ê³ **
---

<br/>

ì‚¬ìš© ì–¸ì–´ëŠ” `python` , êµ¬í˜„ í™˜ê²½ì€  `jupyter notebook` ì…ë‹ˆë‹¤.

```python
import pandas as pd
import numpy as np

# ë°ì´í„° ë¡œë“œ
train=pd.read_csv('data/train.csv', index_col=0)
test=pd.read_csv('data/test_x.csv', index_col=0)
submission=pd.read_csv('data/sample_submission.csv', index_col=0)
# ë²”ì£¼í˜• -> ì´ì‚°í˜•
X = pd.get_dummies(train.drop('voted', axis = 1))
y = train['voted']
test = pd.get_dummies(test)

# ì¹¼ëŸ¼ ê°œìˆ˜ ë³€í™”
print("X : {}\ntest : {}".format(X.shape, test.shape))
# ì¸ì½”ë”© í™•ì¸
print("Encoding Success") if list(X.columns) == list(test.columns) else list(test.columns)

#X : (45532, 100)
#test : (11383, 100)
#Encoding Success
```

ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ í›„ ì´ì „ ê²Œì‹œë¬¼ì—ì„œ ë‹¤ë£¨ì—ˆë˜ ì¸ì½”ë”©ì„ ì§„í–‰í•©ë‹ˆë‹¤.

<br/>

---

<br/>

# **03 ë°ì´í„° ì „ì²˜ë¦¬(2)**
---

<br/>

ë°ì´í„° ë‚´ Nan ê°’ê³¼ ì¤‘ë³µê°’ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

nan ê°’ì„ ì—´ì˜ í‰ê· ìœ¼ë¡œ ë°”ê¾¸ëŠ” `fillna` ë©”ì†Œë“œì™€ ì¤‘ë³µ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” `drop_duplicates`ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.

<br/>

```python
# nan ê°’ ë©”ê¾¸ê¸°
X = X.fillna(X.mean())
# ì¤‘ë³µ ê°’ ì œê±°
X.drop_duplicates(keep='first', inplace = True)
# ë¹„êµ -> nan ì—†ìŒ
X.shape

# (45532, 100)
```

<br/>

ë‹¤ìŒìœ¼ë¡œ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ê°’ì´ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì˜ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì„ ë§ì¶”ê±°ë‚˜ í‰ê· ì„ ë§ì¶”ëŠ” ì‘ì—…ìœ¼ë¡œ ì‚¬ì´í‚·ëŸ° íŒ¨í‚¤ì§€ë¡œ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

MinMaxScalerì„ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.

```python
from sklearn.preprocessing import MinMaxScaler
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ -> ë¯¼ë§¥ìŠ¤/ìŠ¤í…ë‹¤ë“œ ëª¨ë‘ ì„±ëŠ¥ ë¹„ìŠ·í•¨
scaler=MinMaxScaler()
scaler.fit(X)
X=scaler.transform(X)
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë™ì¼ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ
test=scaler.transform(test)
```

ì´ë•Œ ì¤‘ìš”í•œ ì ì€ train ë°ì´í„°ì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ testë°ì´í„°ì— ì ìš©í•´ì„œ ìŠ¤ì¼€ì¼ë§ í•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì‹¤ì œë¡œ Standard ìŠ¤ì¼€ì¼ë§ì™€ Robust ìŠ¤ì¼€ì¼ë§ë„ ì§„í–‰í•˜ì˜€ìœ¼ë‚˜ ì‹¤ì œ ê²°ê³¼ê°’ì—ëŠ” í° ì°¨ì´ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤. ì„¤ë¬¸ ì¡°ì‚¬ ê²°ê³¼ê°€ ì…ë ¥ëœ ë°ì´í„° íŠ¹ì„± ìƒ ë°ì´í„°ì˜ ë¶„í¬ê°€ ì¹¼ëŸ¼ ë³„ë¡œ í° ì°¨ì´ê°€ ì—†ê¸° ë•Œë¬¸ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ë”°ë¼ì„œ ìŠ¤ì¼€ì¼ë§ ì‘ì—…ì€ ìƒëµí•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤. 

â€‹    <br/>

```python
X

#array([[5.00000000e-01, 1.40020340e-04, 7.50000000e-01, ...,
#        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],
#       [1.00000000e+00, 2.57670567e-04, 1.00000000e+00, ...,
#        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
#       [7.50000000e-01, 6.61989656e-04, 0.00000000e+00, ...,
#        0.00000000e+00, 1.00000000e+00, 0.00000000e+00],
#       ...,
#       [7.50000000e-01, 2.35300453e-04, 0.00000000e+00, ...,
#        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
#       [0.00000000e+00, 2.99096703e-04, 5.00000000e-01, ...,
#        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
#       [5.00000000e-01, 1.95117101e-04, 1.00000000e+00, ...,
#        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])
```

ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

  <br/>

---

  <br/>

# 04 ëª¨ë¸ë§ (BayesianOptimization)

  <br/>

Auto ML ë¶„ì•¼ì¸ Hyperparameter Optimization ì¤‘ bayesian optimization ë°©ë²•ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ë§ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

ëª¨ë¸ ì•™ìƒë¸”ì„ ì œì™¸í•˜ê³  ë‹¨ì¼ ëª¨ë¸ë¡œ ìµœì  ê²°ê³¼ê°’ì„ ì—°ì‚°í•˜ê² ìŠµë‹ˆë‹¤. ì‚¬ìš©í•  ëª¨ë¸ì€ LGBMì…ë‹ˆë‹¤.

bayesian-optimizationì„ íŒŒì´ì¬ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ í•´ë‹¹ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. pip ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

-  bayesian-optimization 1.2.0 [ë§í¬](https://pypi.org/project/bayesian-optimization/)

```python
pip install bayesian-optimization
```

`bayes_opt` ëª¨ë“ˆì´ í™˜ê²½ì— ë‹¤ìš´ë¡œë“œ ë˜ë©° ì„¤ì¹˜ê°€ ì™„ë£Œë©ë‹ˆë‹¤.

  <br/>

í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„í¬íŠ¸í•˜ê² ìŠµë‹ˆë‹¤. 

```python
import lightgbm as lgbm
from bayes_opt import BayesianOptimization
from sklearn.metrics import roc_auc_score, make_scorer
from sklearn.model_selection import cross_validate
```

ì‚¬ìš©í•  ëª¨ë¸ì¸ `lgbm` ì™€ `BayesianOptimization` ëª¨ë“ˆì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ì¶”ê°€ë¡œ  ë³¸ ëŒ€íšŒì˜ í‰ê°€ì§€í‘œëŠ” AUCë¡œ auc_scoreì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ì´í‚·ëŸ°ì˜ `roc_auc_score` ëª¨ë“ˆ ë“±ì„ ì„í¬íŠ¸í–ˆìŠµë‹ˆë‹¤.

  <br/>

ë‹¤ìŒì€ ëª©ì í•¨ìˆ˜ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ëª©ì í•¨ìˆ˜ë€ lgbmì˜ ì„±ëŠ¥ í•¨ìˆ˜ë¡œ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì…ë ¥ê°’ìœ¼ë¡œ ê°€ì§‘ë‹ˆë‹¤. ì„±ëŠ¥í‰ê°€ëŠ” aucì´ë¯€ë¡œ scoreê°’ì„ return í•´ ì¤ë‹ˆë‹¤. 

```python
#ëª©ì í•¨ìˆ˜ ìƒì„±
def lgbm_cv(learning_rate, num_leaves, max_depth, min_child_weight, colsample_bytree, feature_fraction, bagging_fraction, lambda_l1, lambda_l2):
    model = lgbm.LGBMClassifier(learning_rate=learning_rate,
                                n_estimators = 300,
                                num_leaves = int(round(num_leaves)),
                                max_depth = int(round(max_depth)),
                                min_child_weight = int(round(min_child_weight)),
                                colsample_bytree = colsample_bytree,
                                feature_fraction = max(min(feature_fraction, 1), 0),
                                bagging_fraction = max(min(bagging_fraction, 1), 0),
                                lambda_l1 = max(lambda_l1, 0),
                                lambda_l2 = max(lambda_l2, 0)
                               )
    scoring = {'roc_auc_score': make_scorer(roc_auc_score)}
    result = cross_validate(model, X, y, cv=5, scoring=scoring)
    auc_score = result["test_roc_auc_score"].mean()
    return auc_score
```

í•¨ìˆ˜ì˜ ì¸ìˆ˜ëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ìµœì  ê°’ì„ íƒìƒ‰í•˜ê³ ì í•˜ëŠ” íŒŒë¦¬ë¯¸í„°ë¥¼ ì…ë ¥í•´ì¤ë‹ˆë‹¤. ì €ëŠ” ì´ 9ê°œì˜ íŒŒë¼ë¯¸í„°ë¡œ ì¡°í•©ì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤. `n_estimators`ì€ í•˜ë“œê°’ìœ¼ë¡œ ê³ ì •í•˜ê³  ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•  ê³„íšì…ë‹ˆë‹¤.  `int(round())` ì€ intê°’ì„ ê°€ì§€ëŠ” íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ì„¤ì •í•˜ê³ , ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì´ ê³ ì •ëœ íŒŒë¼ë¯¸í„°ëŠ” `max`ì™€ `min` í•¨ìˆ˜ë¡œ ë²”ìœ„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. 

`cross_validate`ì€ scoreì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë‹¨ì¼ í‰ê°€ì§€í‘œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° `cross_val_score` ì„ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¼ë°˜ì ì´ì§€ë§Œ, ë‹¤ì¤‘ ì§€í‘œë¥¼ ì‚¬ìš©í•  ê²½ìš°ì— ëŒ€ë¹„í•´ì„œ   `make_scorer` ë©”ì†Œë“œë¡œ roc_auc_score ê°’ì„ ë„ì¶œí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ì¸ cvê°’ì€ 5ë¡œ ì§€ì •í–ˆìŠµë‹ˆë‹¤. 

  <br/>

ë‹¤ìŒì€ ì…ë ¥ê°’, ì¦‰ íŒŒë¼ë¯¸í„°ì˜ íƒìƒ‰ êµ¬ê°„ì„ ì„¤ì •í•©ë‹ˆë‹¤.

```python
# ì…ë ¥ê°’ì˜ íƒìƒ‰ ëŒ€ìƒ êµ¬ê°„
pbounds = {'learning_rate' : (0.0001, 0.05),
           'num_leaves': (300, 600),
           'max_depth': (2, 25),
           'min_child_weight': (30, 100),
           'colsample_bytree': (0, 0.99),
           'feature_fraction': (0.0001, 0.99),
           'bagging_fraction': (0.0001, 0.99),
           'lambda_l1' : (0, 0.99),
           'lambda_l2' : (0, 0.99),
          }
```

íŒŒë¼ë¯¸í„°ì˜ íƒìƒ‰ ë²”ìœ„ëŠ” ëª¨ë¸ ë³„ ì„¤ëª… ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ìƒëµí•˜ê² ìŠµë‹ˆë‹¤.

- learning_rate : ë³´í†µ 0.01~ ì •ë„ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì„¸ë¶€ ì¡°ì •ì„ ìœ„í•´ì„œëŠ” 0.0001~ì •ë„ë¡œ ì„¤ì •í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.

- num_leaves : 250ì •ë„ë¡œ ì„¤ì •í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤. 300~600 ì •ë„ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. 

- max_depth : -1 ë¡œ ì„¤ì •í•˜ë©´ ë¬´í•œëŒ€ë¡œ íŠ¸ë¦¬ê°€ ê¸¸ì–´ì§‘ë‹ˆë‹¤. 9~ ì •ë„ë¡œ ì„¤ì •í•˜ëŠ”ê²Œ ë¬´ë°©í•˜ë‚˜ ì¡°ê¸ˆ ë” ë„“ì€ ë²”ìœ„ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.

- feature_fraction, bagging_fraction  : 0ê³¼ 1 ì‚¬ì´ì˜ ë²”ìœ„ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.

 <br/>

ë‹¤ìŒì€ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 

```python
#ê°ì²´ ìƒì„±
lgbmBO = BayesianOptimization(f = lgbm_cv, pbounds = pbounds, verbose = 2, random_state = 22 )
```

ì²«ë²ˆì§¸ ì¸ìëŠ” ëª©ì í•¨ìˆ˜ fì´ê³ , pboundsëŠ” ì…ë ¥ê°’ì˜ íƒìƒ‰ êµ¬ê°„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. random seedëŠ” 22ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.

 <br/>

Bayesian Optimization ê³¼ì •ì„ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤.

```python
# ë°˜ë³µì ìœ¼ë¡œ ë² ì´ì§€ì•ˆ ìµœì í™” ìˆ˜í–‰
# acq='ei'ì‚¬ìš©
# xi=0.01 ë¡œ explorationì˜ ê°•ë„ë¥¼ ì¡°ê¸ˆ ë†’ì„
lgbmBO.maximize(init_points=5, n_iter = 20, acq='ei', xi=0.01)
```

`init_points`ëŠ” ì²˜ìŒ íƒìƒ‰ íšŸìˆ˜ì…ë‹ˆë‹¤. pboundì—ì„œ ì„¤ì •í•œ êµ¬ê°„ ë‚´ì—ì„œ `init_points` ë§Œí¼ ì…ë ¥ê°’ì„ ìƒ˜í”Œë§í•˜ì—¬ ê³„ì‚°ì´ ì§„í–‰ë©ë‹ˆë‹¤. `n_iter`ì€ ì—°ì‚° íšŸìˆ˜ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ 25ë²ˆì„ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. 

acqëŠ” EIë¡œ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤. xiëŠ” exploration-explotationì˜ ê°•ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ì¸ìˆ˜ë¡œ ì¼ë°˜ì ìœ¼ë¡œ 0.01ë¡œ ì„¤ì •í•˜ì—¬ explorationì„ ë†’ì—¬ì¤ë‹ˆë‹¤.

ì—°ì‚° ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
#|   iter    |  target   | baggin... | colsam... | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_ch... | num_le... |
#-------------------------------------------------------------------------------------------------------------------------------------
#|  1        |  0.6986   |  0.5434   |  0.708    |  0.5968   |  0.5394   |  0.4194   |  0.03233  |  12.06    |  92.42    |  589.1    |
#|  2        |  0.6865   |  0.3797   |  0.7838   |  0.5237   |  0.5624   |  0.9163   |  0.003645 |  4.004    |  31.42    |  549.8    |
#|  3        |  0.7      |  0.7704   |  0.8613   |  0.9688   |  0.7912   |  0.4569   |  0.03905  |  4.72     |  74.79    |  343.0    |
#|  4        |  0.6999   |  0.9352   |  0.5166   |  0.4106   |  0.2619   |  0.7665   |  0.02286  |  15.07    |  31.32    |  485.3    |
#|  5        |  0.6978   |  0.606    |  0.6108   |  0.9343   |  0.675    |  0.3559   |  0.02191  |  18.05    |  34.22    |  500.0    |
#|  6        |  0.6994   |  0.5384   |  0.04871  |  0.3478   |  0.1378   |  0.9633   |  0.0238   |  22.71    |  57.85    |  442.4    |
#|  7        |  0.7003   |  0.615    |  0.2842   |  0.4637   |  0.7906   |  0.5104   |  0.04883  |  3.632    |  30.24    |  382.4    |
#|  8        |  0.6543   |  0.686    |  0.9669   |  0.01047  |  0.1853   |  0.2557   |  0.03188  |  24.52    |  30.22    |  315.8    |
#|  9        |  0.6977   |  0.5413   |  0.8265   |  0.2976   |  0.03282  |  0.2123   |  0.03936  |  2.633    |  70.79    |  384.3    |
#|  10       |  0.6983   |  0.6941   |  0.3692   |  0.3581   |  0.4821   |  0.6529   |  0.01721  |  4.418    |  30.27    |  422.1    |
#|  11       |  0.683    |  0.7734   |  0.4754   |  0.05079  |  0.4677   |  0.4786   |  0.0293   |  2.596    |  31.73    |  383.5    |
#|  12       |  0.6961   |  0.8518   |  0.4633   |  0.3379   |  0.6867   |  0.06141  |  0.04023  |  12.76    |  91.87    |  545.5    |
#|  13       |  0.6994   |  0.9603   |  0.003919 |  0.4968   |  0.1905   |  0.3403   |  0.02221  |  14.29    |  31.24    |  485.8    |
#|  14       |  0.6991   |  0.3036   |  0.06767  |  0.4564   |  0.806    |  0.5681   |  0.0316   |  5.087    |  72.1     |  343.6    |
#|  15       |  0.701    |  0.159    |  0.2529   |  0.7675   |  0.7801   |  0.6409   |  0.02265  |  8.193    |  30.48    |  380.3    |
#|  16       |  0.6995   |  0.7101   |  0.2929   |  0.7939   |  0.481    |  0.2528   |  0.0371   |  6.158    |  30.51    |  382.0    |
#|  17       |  0.6792   |  0.7479   |  0.3636   |  0.888    |  0.009658 |  0.9869   |  0.00187  |  7.997    |  76.45    |  346.3    |
#|  18       |  0.694    |  0.119    |  0.4861   |  0.1017   |  0.8208   |  0.01521  |  0.03531  |  5.709    |  30.5     |  376.8    |
#|  19       |  0.6676   |  0.8015   |  0.234    |  0.01009  |  0.2333   |  0.1733   |  0.04348  |  2.493    |  73.36    |  341.4    |
#|  20       |  0.6985   |  0.1058   |  0.7954   |  0.2442   |  0.05641  |  0.74     |  0.03221  |  8.176    |  31.4     |  381.6    |
#|  21       |  0.7043   |  0.9516   |  0.786    |  0.9362   |  0.7111   |  0.5643   |  0.01141  |  6.09     |  33.33    |  380.2    |
#|  22       |  0.7016   |  0.9625   |  0.4699   |  0.8483   |  0.598    |  0.06879  |  0.02233  |  6.539    |  35.06    |  379.3    |
#|  23       |  0.6949   |  0.03632  |  0.1783   |  0.4311   |  0.04416  |  0.3632   |  0.005125 |  6.761    |  32.44    |  378.9    |
#|  24       |  0.7007   |  0.4155   |  0.2127   |  0.6469   |  0.8476   |  0.9304   |  0.02003  |  6.782    |  33.45    |  382.1    |
#|  25       |  0.6987   |  0.3296   |  0.6078   |  0.9726   |  0.005871 |  0.1608   |  0.004235 |  7.953    |  37.69    |  381.7    |
#=====================================================================================================================================
```

`target` ê°’ì€ ëª©ì í•¨ìˆ˜ì˜ return ê°’ìœ¼ë¡œ auc scoreì— í•´ë‹¹í•©ë‹ˆë‹¤. ì•½ ìµœëŒ€ 0.7ì •ë„ì˜ ê°’ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. 

í¬ê²Œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ, ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì¶”ì •í•˜ê±°ë‚˜ ëª¨ë¸ ì•™ìƒë¸”ì„ ì§„í–‰í•œë‹¤ë©´ ë”ìš± ì¢‹ì€ scoreì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

 <br/>

ê³„ì‚°í•œ íŒŒë¼ë¯¸í„° ê°’ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
# ì°¾ì€ íŒŒë¼ë¯¸í„° ê°’ í™•ì¸
lgbmBO.max

#{'target': 0.7042782365476333,
# 'params': {'bagging_fraction': 0.9515683153667026,
#  'colsample_bytree': 0.7860120233288207,
#  'feature_fraction': 0.9362075859090412,
#  'lambda_l1': 0.7111030183072032,
#  'lambda_l2': 0.5642765168754059,
#  'learning_rate': 0.011407920284082697,
#  'max_depth': 6.090247578634,
#  'min_child_weight': 33.33272426081254,
#  'num_leaves': 380.181539974917}}
```

 <br/>

í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤. 

```python
#íŒŒë¼ë¯¸í„° ì ìš©
fit_lgbm = lgbm.LGBMClassifier(learning_rate=lgbmBO.max['params']['learning_rate'],
                               num_leaves = int(round(lgbmBO.max['params']['num_leaves'])),
                               max_depth = int(round(lgbmBO.max['params']['max_depth'])),
                               min_child_weight = int(round(lgbmBO.max['params']['min_child_weight'])),
                               colsample_bytree=lgbmBO.max['params']['colsample_bytree'],
                               feature_fraction = max(min(lgbmBO.max['params']['feature_fraction'], 1), 0),
                               bagging_fraction = max(min(lgbmBO.max['params']['bagging_fraction'], 1), 0),
                               lambda_l1 = lgbmBO.max['params']['lambda_l1'],
                               lambda_l2 = lgbmBO.max['params']['lambda_l2']
                               )

model = fit_lgbm.fit(X,y)
```

ëª¨ë¸ ì—°ì‚°ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

 <br/>

---

# 05 ëª¨ë¸ ì ìš©

  <br/>

ìš°ì„  ëª¨ë¸ì„ ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤. `joblib`ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì„ .pklë¡œ ì‰½ê²Œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import joblib
joblib.dump(model, 'model.pkl')
```

ëª¨ë¸ì„ ì €ì¥í•˜ë©´ ì•™ìƒë¸”ì„ ì§„í–‰í•˜ê¸° ìˆ˜ì›”í•©ë‹ˆë‹¤. 

  <br/>

test ë°ì´í„° ì…‹ë„ ëª¨ë¸ì— ì ìš©í•˜ì—¬ ìµœì¢… ë¼ë²¨ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

````python
pred_y = model.predict(test)

submission['voted']=pred_y
submission.to_csv('submission.csv')
````

submission.csvë¡œ ê³„ì‚° ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.

  <br/>

---

  <br/>

# **ğŸ“— ì •ë¦¬**

---

 <br/>

ì´ë²ˆ ê²Œì‹œë¬¼ì—ì„œ ì†Œê°œí•œ ë°©ë²• ì™¸ì—ë„ ë‹¤ì–‘í•œ auto ml ë°©ë²•ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì—°ì‚°í•œ í›„ scoreê°€ ì¢‹ì€ ëª¨ë¸ì„ ì•™ìƒë¸” í•˜ë©´ ë”ìš± ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ì•„ë˜ ë§í¬ì—ì„œ í•´ë‹¹ ë‚´ìš©ì˜ ë…¸íŠ¸ë¶ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- modeling_BO.ipynb [ë§í¬](https://github.com/Jihyun22/Dacon_workspace/blob/master/psychology/mode_BO.ipynb)

ì´ì–´ì§€ëŠ” ê²Œì‹œë¬¼ì—ì„œëŠ” ë°ì´í„° EDAë¥¼ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. 

 <br/>
