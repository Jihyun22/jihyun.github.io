{"pages":[{"title":"about","text":"About test","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"[LeetCode] Best Time to Buy and Sell Stock III","text":"LeetCoding Challenge의 8월 16일 ‘Best Time to Buy and Sell Stock III’ 문제 풀이입니다. August LeetCoding Challenge week-3-august-15th-august-21st 🎯 문제 Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete at most two transactions. Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). 주식의 가격이 날짜별로 주어지면 주식을 사고 팔아서(transaction) 얻을 수 있는 최대 이익을 찾아내는 문제입니다. transaction은 중복으로 이루어질 수 없으며, 최대 2번입니다. Test Case11234Input: [3,3,5,0,0,3,1,4]Output: 6Explanation: 4일날 주식을 구입하여 6일날 팔고(이익(3-0) = 3), 7일날 구입하여 8일날 팔아서(이익(4-1)=3) 최대 이익은 6 입니다. Note : transaction은 최대 2번 이루어진다는 점에 유의해야 합니다. Test Case21234Input: [1,2,3,4,5]Output: 4Explanation: 1일날 주식을 구입하여 5일날 팔아서(이익(5-1)=4) 최대 이익은 4입니다.Note : transaction은 중복해서 이루어지지 않는다는 점에 유의해야 합니다(병행 불가). Test Case312Input: [7,6,4,3,1]Output: 0 🔍 풀이해설첫번째 테스트 케이스를 예시로 설명하겠습니다. 첫번째 transaction첫번째 transaction은 왼쪽에서 오른쪽으로 이익을 계산합니다. S_min은 stock[:i] 의 최소값이고, 최대 이익은 stock[i] - S_min 의 최대값이 저장됩니다. stock 3 3 5 0 0 3 1 4 최대 이익(tmp1) 0 0 2 2 2 3 3 4 S_min 3 3 3 0 0 0 0 0 두번째 transaction두번째 transaction은 오른쪽에서 왼쪽으로 이익을 계산합니다. S_max은 stock[n-i:n] 의 최대값이고, 최대 이익은 S_max - stock[i]의 최대값이 저장됩니다. stock 3 3 5 0 0 3 1 4 최대 이익(tmp2) 4 4 4 4 4 3 3 0 S_max 5 5 5 4 4 4 4 4 최대 이익 계산이때 tmp1[i] + tmp2[i]은 가능한 이익의 경우의 수입니다. 이익(tmp1[i]+tmp2[i]) 4 4 6 6 6 6 6 4 최대 이익(tmp1) 0 0 2 2 2 3 3 4 최대 이익(tmp2) 4 4 4 4 4 3 3 0 따라서 결과값은 tmp1[i] + tmp2[i]의 최대값으로 계산합니다. 코드사용 언어는 python3입니다. 12345678910111213141516171819202122232425262728293031323334# python# [LeetCode] week-3-august-16th# @Jihyun22# input -&gt; 괄호 제거, ','기준으로 int값 분할하여 stock이름의 list 생성stock = list(map(int, input().strip('[]').split(',')))def maxProfit(stock): day = len(stock) # 입력값이 2개 미만이면 0 return if day &lt; 2: return 0 # 양 끝값에서 중앙으로 탐색 left, right = stock[0], stock[day-1] # 최대 트렌젝션은 2번 일어나므로 각각 이익 저장할 배열 선언 tmp1, tmp2 = [0]*day, [0]*day for i in range(1, day): # 왼쪽 -&gt; 오른쪽 순으로 S_min 연산 tmp1[i] = max(tmp1[i-1], stock[i]-left) left = min(left, stock[i]) # 오른쪽 -&gt; 왼쪽 순으로 S_max 연산 j = day-1-i tmp2[j] = max(tmp2[j+1], right-stock[j]) right = max(right, stock[j]) # 이익은 음수가 될 수 없으므로 초기값은 0 result = 0 # 각각의 tmp에서 i번째 수의 합은 이익의 경우의 수 for i in range(day): # 이익의 최대값 연산 result = max(result, tmp1[i]+tmp2[i]) return resultprint(maxProfit(stock)) 📝 SubmitLeetCode 제출 코드입니다. 1234567891011121314151617class Solution: def maxProfit(self, stock: List[int]) -&gt; int: day = len(stock) tmp1 = [0 for i in range(day)] tmp2 = [0 for i in range(day)] cache = [[0 for i in range(day)] for i in range(day)] for i in range(0, day): for j in range(i, day): cache[i][j] = stock[j]-stock[i] for i in range(day-1, -1, -1): tmp1[i] = max(cache[i][0:i+2]) tmp2[i] = max(cache[0:i+2][i]) tmp1.sort(reverse=True) return max(sum(tmp1[0:2]), max(tmp2)) LeetCode는 사용 언어 별 default 형식으로 작성해야 평가가 진행됩니다. 관련 카테고리 포스트 더보기 Algorithm 관련 포스트 더보기 Leetcode 관련 포스트 더보기","link":"/2020/08/16/leetcode-08-16/"},{"title":"[LeetCode] Distribute Candies to People","text":"LeetCoding Challenge의 8월 17일 ‘Distribute Candies to People’ 문제 풀이입니다. August LeetCoding Challenge week-3-august-15th-august-21st 🎯 문제 We distribute some number of candies, to a row of n = num_people people in the following way: We then give 1 candy to the first person, 2 candies to the second person, and so on until we give n candies to the last person. Then, we go back to the start of the row, giving n + 1 candies to the first person, n + 2 candies to the second person, and so on until we give 2 * n candies to the last person. This process repeats (with us giving one more candy each time, and moving to the start of the row after we reach the end) until we run out of candies. The last person will receive all of our remaining candies (not necessarily one more than the previous gift). Return an array (of length num_people and sum candies) that represents the final distribution of candies. Constraints: 1 &lt;= candies &lt;= 10^9 1 &lt;= num_people &lt;= 1000 캔디의 개수와 사람의 수가 주어지고, 사람에게 캔디를 분배하는 문제입니다. 사람에게 캔디를 1, 2, 3 …개 순서로 분배하며, 마지막 사람까지 나눠준 다음 사탕이 남으면 다시 줄의 시작으로 돌아가 n+1, n+2 … 순으로 재 분배합니다. 사탕이 다 떨어질 때 까지 이런 과정이 반복되며, 마지막 사람은 남은 사탕을 받습니다. 이때, 기존 사탕 분배 개수보다 반드시 한개 더 많이 받을 필요는 없습니다. 결과값은 사탕의 최종 분포를 리스트로 반환합니다. Test Case112Input: candies = 7, num_people = 4Output: [1,2,3,1] Test Case212Input: candies = 10, num_people = 3Output: [5,2,3] 🔍 풀이해설1, 2, 3, 4 … 배열의 누적 합 배열 1, 3, 6, 10 … 을 고려하면 쉽게 접근할 수 있습니다. 사람의 수 만큼의 열을 가진 2차원 배열을 선언한 후, 사탕을 순서대로 분배하여 각 요소의 합을 연산해 결과값을 도출합니다. Test case 2 j=0 j=1 j=2 i=0 1 (1) 2 (3) 3 (6) i=1 4 (10) result 5 2 3 코드사용 언어는 python3입니다. 1234567891011121314151617181920212223242526272829# python# [LeetCode] week-3-august-17th# Distribute Candies to People# @Jihyun22# 사탕 수와 사람 수가 각각 공백을 기준으로 입력되는 경우c, p = list(map(int, input().split(&quot; &quot;)))result = [0] * p# candy_sum이 처음으로 10**9 이상일 때 i는 45candy = 1candy_sum = 0tmp = [[0] * p for _ in range(45)]for i in range(45): for j in range(p): if candy_sum + candy &gt; c: tmp[i][j] = c - candy_sum candy = -1 result[j] += tmp[i][j] break tmp[i][j] = candy candy_sum += candy candy += 1 result[j] += tmp[i][j] # 이중 for 문 종료 if candy &lt; 0: breakprint(result) 📝 SubmitLeetCode 제출 코드입니다. 1234567891011121314151617181920class Solution: def distributeCandies(self, candies: int, num_people: int) -&gt; List[int]: result = [0] * num_people candy = 1 candy_sum = 0 tmp = [[0] * num_people for _ in range(45)] for i in range(45): for j in range(num_people): if candy_sum + candy &gt; candies: tmp[i][j] = candies - candy_sum candy = -1 result[j] += tmp[i][j] break tmp[i][j] = candy candy_sum += candy candy += 1 result[j] += tmp[i][j] if candy &lt; 0: break return result LeetCode는 사용 언어 별 default 형식으로 작성해야 평가가 진행됩니다. 채점 결과 채점 결과, 가장 보편적인 방법으로 접근한 것으로 판단됩니다. 상위 코드를 통해 run time을 줄일 수 있는 방법을 고민해봐야겠습니다. 아래에 run time = 20ms인 상위 코드 중 일부를 첨부합니다. 123456789101112131415161718lo, hi = 0, candies K = 0 while lo &lt;= hi: k = (lo + hi)//2 if k*(num_people*(num_people+1))//2 + (k*(k-1))//2 * num_people**2 &lt;= candies: K = k lo = k + 1 else: hi = k - 1 result = [(i+1)*K+num_people*(K*(K-1))//2 for i in range(num_people)] candies -= sum(result) for i in range(num_people): add = min(candies, K * num_people + i + 1) result[i] += add candies -= add if candies == 0: break return result 관련 카테고리 포스트 더보기 Algorithm 관련 포스트 더보기 Leetcode 관련 포스트 더보기","link":"/2020/08/17/leetcode-08-17/"},{"title":"[LeetCode] Numbers With Same Consecutive Differences","text":"LeetCoding Challenge의 8월 18일 ‘Numbers With Same Consecutive Differences’ 문제 풀이입니다. August LeetCoding Challenge week-3-august-15th-august-21st 🎯 문제 Return all non-negative integers of length N such that the absolute difference between every two consecutive digits is K. Note that every number in the answer must not have leading zeros except for the number 0 itself. For example, 01 has one leading zero and is invalid, but 0 is valid. You may return the answer in any order. Note: 1 &lt;= N &lt;= 9 0 &lt;= K &lt;= 9 N은 자릿수, K는 연속된 두 자리 수 사이의 절대 차이로 N과 K를 만족하는 양의 정수(0을 포함하는 자연수)를 반환하는 문제입니다. 이때 반환 순서는 고려할 필요 없으며, 맨 첫자리에 0이 오는 경우는 제외합니다 (01은 유효하지 않음, 0은 유효함). Test Case112Input: N = 3, K = 7Output: [181,292,707,818,929] Test Case212Input: N = 2, K = 1Output: [10,12,21,23,32,34,43,45,54,56,65,67,76,78,87,89,98] 🔍 풀이해설이 문제의 edge event는 N=1인 경우의 수 입니다. 이때 결과값은 range(10) 으로 0이 포함됩니다. 따라서 사전에 edge event를 처리해 주는 것이 문제 풀이의 해법이 될 수 있습니다. 자리수 N을 고려하여 숫자를 조합하는 것도 까다롭습니다. 이때 a*10 + b 꼴에서 a에 10의 자리의 수가 들어가면 100의 자리의 수가 된다는 점을 고려한다면 보다 쉽게 접근할 수 있습니다. 또한 첫째 자리 수(a*10 + b 에서 a)가 0이 되면 제외된다는 조건은 if a 로 쉽게 처리할 수 있습니다. 자리수 N 연산첫번째 테스트 케이스(N=3, K=7)를 예시로 설명하겠습니다. 123456```pythonresult = {a * 10 + b for a in result for b in [a % 10 + K, a % 10 - K] if a and 0 &lt;= b &lt; 10} result의 값은 다음의 표와 같이 계산됩니다. 0 to N-1 a b result 0 1 8 18, 2, 3, 4, 5, 6, 7, 8, 9 0 2 9 18, 29, 3, 4, 5, 6, 7, 8, 9 0 7 0 18, 29, 70, 8, 9 0 8 1 18, 29, 70, 81, 9 0 9 2 18, 29, 70, 81, 92 1 18 1 181, 29, 70, 81, 92 1 29 2 292, 181,70, 81, 92 1 70 7 707, 292, 181, 81, 92 1 81 8 818, 707, 292, 181, 92 1 92 9 929, 818, 707, 292, 181 코드사용 언어는 python3입니다. 12345678910111213141516171819202122232425# python# [LeetCode] week-3-august-18th# Numbers With Same Consecutive Differences# @Jihyun22# N과 K가 공백 기준으로 입력되는 경우N, K = list(map(int, input().split(&quot; &quot;)))def sol(N, K): # 0부터 9까지의 result에 저장 result = range(10) # 자리수 만큼 반복 # N=1이면 result = range(10) 반환(edge event) for i in range(N - 1): result = { # 자리수 고려 a * 10 + b # result에 저장된 수 만큼 a연산 반복 for a in result # a, b의 절대 차는 K for b in [a % 10 + K, a % 10 - K] # a에는 0이 올 수 없으며, b는 0~9 까지의 자연수 if a and 0 &lt;= b &lt; 10} return list(result)print(sol(N, K)) 📝 SubmitLeetCode 제출 코드입니다. 123456789class Solution: def numsSameConsecDiff(self, N: int, K: int) -&gt; List[int]: result = range(10) for i in range(N - 1): result = {a * 10 + b for a in result for b in [a % 10 + K, a % 10 - K] if a and 0 &lt;= b &lt; 10} return list(result) LeetCode는 사용 언어 별 default 형식으로 작성해야 평가가 진행됩니다. 채점 결과 채점 결과, run time은 상위 0.5%정도로 좋은 성과를 얻을 수 있었습니다. 다만 메모리 측면에서는 아쉬움이 남습니다. 재귀법을 사용한 코드가 메모리 효율이 가장 높았습니다. 해당 코드의 일부를 첨부합니다. 123456789101112131415161718if N == 1: return list(range(10)) output = []def addRecursive(digits, N, K, output): if len(digits) == N: x = 0 for d in digits: x = 10 * x + d output.append(x) else: y = digits[-1] if y &gt;= K: addRecursive(digits + [y-K], N, K, output) if K &gt; 0 and y+K &lt;= 9: addRecursive(digits + [y+K], N, K, output) for k in range(1, 10): addRecursive([k], N, K, output) return output 추가로 덧붙이면, 이 문제는 보편적인 dfs 문제 유형입니다. 기회가 된다면 dfs로 풀이하는 것도 큰 도움이 될 것 같습니다. dfs 풀이 코드도 첨부하겠습니다. 123456789101112131415161718class Solution: def numsSameConsecDiff(self, N: int, k: int) -&gt; List[int]: ans = list() if N == 1: ans.append(0) def dfs(n,num): if n == 0: ans.append(num) return t = num%10 digits = {t+k} digits.add(t-k) for d in digits: if -1 &lt; d &lt; 10: dfs(n-1,(num*10)+d) for x in range(1,10): dfs(N-1,x) return ans 관련 카테고리 포스트 더보기 Algorithm 관련 포스트 더보기 Leetcode 관련 포스트 더보기","link":"/2020/08/18/leetcode-08-18/"},{"title":"[LeetCode] Goat Latin","text":"LeetCoding Challenge의 8월 19일 ‘Goat Latin’ 문제 풀이입니다. August LeetCoding Challenge week-3-august-15th-august-21st 🎯 문제 A sentence S is given, composed of words separated by spaces. Each word consists of lowercase and uppercase letters only. We would like to convert the sentence to “Goat Latin” (a made-up language similar to Pig Latin.) The rules of Goat Latin are as follows: If a word begins with a vowel (a, e, i, o, or u), append &quot;ma&quot; to the end of the word.For example, the word ‘apple’ becomes ‘applema’. If a word begins with a consonant (i.e. not a vowel), remove the first letter and append it to the end, then add &quot;ma&quot;.For example, the word &quot;goat&quot; becomes &quot;oatgma&quot;. Add one letter 'a' to the end of each word per its word index in the sentence, starting with 1.For example, the first word gets &quot;a&quot; added to the end, the second word gets &quot;aa&quot; added to the end and so on. Return the final sentence representing the conversion from S to Goat Latin. 문장을 입력받아 “ “공백을 기준으로 한 단어 단위로 나눕니다. 각 단어의 앞글자가 모음인 경우, “ma”를 붙이며, 자음인 경우 앞글자를 제거한 뒤 “ma”를 붙입니다. 또한 문장의 단어 순서대로 차례대로 누적하여 “a”를 붙이고, 최종 결과를 문장으로 반환합니다. Test Case112Input: &quot;I speak Goat Latin&quot;Output: &quot;Imaa peaksmaaa oatGmaaaa atinLmaaaaa&quot; Test Case212Input: &quot;The quick brown fox jumped over the lazy dog&quot;Output: &quot;heTmaa uickqmaaa rownbmaaaa oxfmaaaaa umpedjmaaaaaa overmaaaaaaa hetmaaaaaaaa azylmaaaaaaaaa ogdmaaaaaaaaaa&quot; 🔍 풀이해설문제의 난이도가 낮아 자세한 설명은 아래의 코드 및 주석으로 대체하겠습니다. 코드사용 언어는 python3입니다. 123456789101112131415161718# python# [LeetCode] week-3-august-18th# Goat Latin# @Jihyun22# 문장을 입력받아 공백을 기준으로 list에 저장S = list(map(str, input().split(' ')))for i in range(0, len(S)): # 맨 앞 문자가 자음이면 if S[i][0].lower() not in ['a', 'e', 'i', 'o', 'u']: # 앞 문자를 뒤로 add S[i] = S[i][1:] + S[i][0] # &quot;ma&quot; add # 'a'를 공백 수+1 만큼 add S[i] += &quot;ma&quot; + &quot;a&quot;*(i+1)print(' '.join(S).strip(&quot; &quot;)) 📝 SubmitLeetCode 제출 코드입니다. 123456789class Solution: def toGoatLatin(self, S: str) -&gt; str: S = list(map(str, S.split(' '))) for i in range(0, len(S)): if S[i][0].lower() not in ['a', 'e', 'i', 'o', 'u']: S[i] = S[i][1:] + S[i][0] S[i] += &quot;ma&quot;+&quot;a&quot;*(i+1) return ' '.join(S).strip(&quot; &quot;) LeetCode는 사용 언어 별 default 형식으로 작성해야 평가가 진행됩니다. 채점 결과 채점 결과, run time은 중-하위 수준이였습니다. 상위 코드를 살펴보니 조금 단순하게 접근한 것이 특징이었습니다. 해당 코드를 첨부하겠습니다. 12345678910111213141516171819202122class Solution: def toGoatLatin(self, S: str) -&gt; str: W = S.split(&quot; &quot;) out = '' for i, s in enumerate(W): if s[0].lower() in ['a', 'e', 'i', 'o', 'u']: s = s + &quot;ma&quot; else: s = s[1:] + s[0] s = s + &quot;ma&quot; while(i&gt;=0): s = s + 'a' i = i-1 out = out + s + &quot; &quot; print(W) return out[0:len(out)-1] 메모리 측면에서는 효율이 높은 편입니다. 관련 카테고리 포스트 더보기 Algorithm 관련 포스트 더보기 Leetcode 관련 포스트 더보기","link":"/2020/08/19/leetcode-08-19/"},{"title":"[데이콘 온도추정 대회] 데이터셋 구조 살펴보기","text":"데이콘에서 20년 3-4월 진행된 AI프렌즈 시즌1 온도 추정 경진대회에 대해 리뷰하며, 대회에서 제공하는 데이터셋의 구조와 기본 아이디어에 대해 알아보겠습니다. 0. 배경데이콘은 데이터 분석, 또는 ML에 대한 대회를 다루는 케글과 비슷한 국내 플랫폼입니다. 앞으로 데이콘의 지난 대회를 리뷰하며 ML에 대한 기본적인 개념을 다루고자 합니다. 가장 먼저 살펴볼 대회는 올해 2020년 3월부터 4월 중순까지 진행된 AI프렌즈 시즌1 온도 추정 경진대회입니다. 이 대회는 전국에 걸쳐 시도별 기상 관측소가 있지만, 각 지역 내에서도 대상과 위치에 따라 온도 차이가 나는 점을 개선하고자 기획되었습니다. 저가의 센서로 관심 대상의 온도를 단기간 측정하여 기상청의 관측 데이터와 상관관계 모델을 만들고 온도를 추정하여 서비스하는 것이 최종 목표입니다. 대회에 대한 자세한 정보는 아래의 링크를 참조하세요. AI프렌즈 시즌1 온도 추정 경진대회 1. 데이터 구성1.1 데이터 설명데이콘에서는 각 대회의 데이터 구성 관련 설명 영상을 제공하고 있습니다. 이번 대회에서 사용하는 데이터셋이 어떻게 구성되어 있는지 자세한 설명은 아래의 영상을 참고하세요. 본 대회에서 사용하는 데이터셋은 DACON 링크에서 다운받을 수 있습니다. 1.2. traintrain 데이터부터 살펴보겠습니다. train.csv 파일에는 총 33일 간 기상청 데이터와 온도 센서 데이터 값이 저장되어 있습니다. 123train = pd.read_csv('train.csv', index_col=False,)train = train.drop(['id'], axis=1)train.shape (4752, 59) 우선 기상청 데이터는 강수량, 기온, 기압 등을 포함한 X00 ~ X39 총 40개의 피처로 주어집니다. 33일간 누락된 값은 없습니다. 온도 센서 데이터 값은 Y00 ~ Y18 총 19개가 존재합니다. train.shape 의 열 값이 59인 이유는 40개의 기상청 데이터와 19개의 온도 센서 데이터 값으로 구성되어 있기 때문입니다. 이제 각 피처의 누락값 범위를 확인해보겠습니다. 12# 데이터 처음 5번째 줄 출력train.head() 12# 데이터 끝 5번째 줄 출력train.tail() Y00 ~ Y17 까지의 측정값은 30일간의 측정값만 존재하고, Y18 값은 30일 이후의 3일 간의 데이터만 있습니다. 우리가 최종 예측해야 하는 값은 Y18 값입니다. 1.3. testtest데이터를 살펴보겠습니다. train.csv는 train.csv 기간 이후 80일 간의 기상청 데이터가 주어집니다. 123test = pd.read_csv('test.csv', index_col=False,)test = test.drop(['id'], axis=1)test.shape (11520, 40) 누락값은 없습니다. 12# 데이터 처음 5번째 줄 출력test.head() 표로 정리하면 다음과 같습니다. 구분 train - 30일 train - 3일 test - 80일 Y00 ~ Y17 (18개) 공개 비공개 비공개 Y18 비공개 공개 목표값 X00 ~ X39 (40개) 공개 공개 공개 이러한 데이터의 특징을 바탕으로, 다음 포스트에서는 모델 구성을 위한 기본 아이디어를 고민하겠습니다. 관련 카테고리 포스트 더보기 Machine-Learning 관련 포스트 더보기 Dacon 관련 포스트 더보기 Dacon 온도추정 대회 관련 포스트 더보기","link":"/2020/03/14/temperature-forecast-01/"},{"title":"[데이콘 온도추정 대회] 모델 적용하기","text":"데이콘에서 20년 3-4월 진행된 AI프렌즈 시즌1 온도 추정 경진대회에 대해 리뷰하며, 이전에 다룬 내용을 바탕으로 기본 아이디어와 LSTM 모델 적용 결과를 알아보겠습니다. 1편과 이어지는 자료입니다. 자세한 내용은 아래를 참고해주세요. [데이콘 온도추정 대회] 데이터셋 구조 살펴보기 2. 모델 데모2.1. 기본 아이디어앞에서 살펴본 데이터셋의 특징은 크게 두가지입니다. 데이터의 크기가 상당히 작다. 기상청 데이터의 피처는 40개, 온도 센서 측정값까지 더하면 60개 남짓입니다. 일자 별 누락된 데이터가 있다. test 의 80일치 기상청 데이터를 예측해야 하는데, Y18 은 3일치의 데이터만 주어져 있습니다. 따라서 적은 데이터셋으로 높은 정확도의 정보를 어떻게 이끌어 낼 것인가가 중요한 요점입니다. 이러한 특징을 바탕으로 기본 아이디어를 설계해보겠습니다. 위 표에서 누락된 데이터가 있는 영역을 나누어 (가) ~ (자) 총 9개의 섹션으로 구분했습니다. 구분 train - 30일 train - 3일 test - 80일 Y00 ~ Y17 (18개) (가) 공개 (라) 비공개 (사) 비공개 Y18 (나) 비공개 (마) 공개 (아) 목표값 X00 ~ X39 (40개) (다) 공개 (바) 공개 (자) 공개 X (다) + y (가) 모델1 학습 30일 간의 X00 ~ X39 의 기상청 데이터 값으로 30일 간의 Y00 ~ Y17 의 온도 센서 측정 값을 학습시켜 모델1을 만듭니다. 모델1 : X (바) → y (라) 예측 3일 간의 X00 ~ X39 의 기상청 데이터 값으로 3일 간의 Y00 ~ Y17 의 온도 센서 측정 값을 예측할 수 있습니다. X (바+라) + y (마) 모델2 학습 3일 간의 X00 ~ X39 의 기상청 데이터 값과 3일 간의 Y00 ~ Y17 의 온도 센서 측정 값으로 Y18 의 온도 센서 측정 값을 학습시켜 모델2을 만듭니다. 모델1 : X (자) → y (사) 예측 80일 간의 X00 ~ X39 의 기상청 데이터 값으로 80일 간의 Y00 ~ Y17 의 온도 센서 측정 값을 예측할 수 있습니다. 모델2 : X (자+사) → y (아) 예측 80일 간의 X00 ~ X39 의 기상청 데이터 값과 80일 간의 Y00 ~ Y17 의 온도 센서 측정 값으로 80일 간의 Y18 의 온도 센서 측정 값을 예측할 수 있습니다. 작은 데이터 크기를 보정하기 위해 모델 2는 기상청 데이터 뿐 아니라 Y00 ~ Y17 의 온도 센서 측정 값을 포함시켜 학습하고자 합니다. 따라서 목표값인 (아) 예측 성능은 (사)의 예측 정확도에 좌우될 수 있다는 점에 유의해야 합니다. 2.2. 모델 선정기본 아이디어를 바탕으로 모델 데모를 작성하기 위해 머신러닝 모델을 선정하겠습니다. 예측 값이 2차원인 모델 1은 3차원 분석이 가능한 LSTM이 적절하다고 판단됩니다. 모델2는 예측 값이 Y18 하나로 1차원이기에 다양한 모델이 선호될 수 있습니다. 우선 데모를 위해 간단하게 모델 1과 모델 2 모두 LSTM 를 적용해보겠습니다. 2.3 모델 학습 전체적인 코드는 링크를 참조해주세요. 로컬에서 Jupyter 로 작업하였으며 workplace 내 train.csv, test.csv 를 위치시켰습니다. 2.3.1 데이터 로드우선 데이터를 로드하겠습니다. 모델1을 만들기 위한 데이터 구성입니다. 12345678910111213import pandas as pdimport numpy as npfrom tqdm import tqdm# 트레인셋train = pd.read_csv('train.csv', index_col=False)X_train = train.loc[:,'X00':'X39']y_train = train.loc[:,'Y00':'Y17']# 테스트 셋test = pd.read_csv('test.csv', index_col=False)test=test.drop(['id'], axis=1) 기상청 데이터값을 X_train 으로, Y00 ~ Y17 온도 센서 측정 값을 y_train 으로 로드했습니다. 12345# nan 값 제거y = y_train.dropna()# 트레인 셋 범주 조정X = X_train.loc[:y.shape[0]-1,:] 이후, y_train의 결측치를 제거하여 30일 간의 Y00 ~ Y17 온도 센서 측정 값을 y 로 저장했습니다. 모델 구축을 위해 33일 간의 기상청 데이터로 구성된 X_train 도 y 와 같은 수의 row를 가진 X로 조정합니다. 이제 데이터 프레임 X 와 y 는 각각 30일 간의 기상청 데이터와 온도 센서 측정 값으로 구성되어 있습니다. X.shape : (4320, 40) y.shape : (4320, 18) 2.3.2 모델 1 학습다음은 LSTM모델을 세팅하겠습니다. 모든 파라미터는 default 값입니다. 12345678910111213from keras.layers import LSTMfrom keras.models import Sequentialfrom keras.layers import Denseimport keras.backend as Kfrom keras.callbacks import EarlyStoppingK.clear_session()model_1 = Sequential() # Sequeatial Modelmodel_1.add(LSTM(20, input_shape=(X.shape[1], 1))) # 트레인값model_1.add(Dense(y.shape[1])) # 출력값model_1.compile(loss='mean_squared_error', optimizer='adam')model_1.summary() LSTM 모델에 사용되는 학습 데이터와 출력값의 크기를 세팅하였습니다. 이제 세팅한 학습 데이터의 크기에 맞게 X 를 조정하겠습니다. 12X = X.valuesX = X.reshape(X.shape[0], X.shape[1], 1) pandas dataframe 을 LSTM의 학습 데이터로 사용하기 위해서는 .values 로 dataframe 을 풀어준 후, .reshape 메소드로 (x, y, z) 값을 설정하면 됩니다. 모델 학습을 진행하겠습니다. 일정 이상 loss값에 도달하면 early stop 되며 다른 파라미터는 기본값 세팅으로 사용했습니다. 1234early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1)model_1.fit(X, y, epochs=10000, batch_size=30, verbose=1, callbacks=[early_stop]) 최종 loss값 5.869으로 학습을 마무리했습니다. 모델1은 기본 아이디어에 따라 이후 사용됩니다. 재사용의 편의를 위해 모델을 저장하고 적용 함수를 만들었습니다. 123456789101112#모델 저장import joblibjoblib.dump(model_1, 'model_1.pkl')# 모델 적용 함수def model_fit_1(data, model_name, file_name): model = joblib.load(str(model_name)) data_r = data.values.reshape(data.shape[0],data.shape[1],1 ) pred_out=model.predict(data_r) df = pd.DataFrame(pred_out) df.to_csv(str(file_name), index = False, header = False) return df 이 모델로 지난 3일간의 y 값 (라)을 연산했습니다. 12345#지난 3일간의 데이터셋X_test = X_train.loc[y.shape[0]:,:]#지난3일간의 y값 연산pred_out = model_fit_1(X_test, 'model_1.pkl', 'pred_out_1.csv') 2.3.3. 모델 2 학습이제 X (바+라) + y (마) 모델2 학습 단계입니다. 1234X=pd.concat([X_test.reset_index(drop=True), pred_out.reset_index(drop=True)], axis=1)y = train['Y18']y = y.dropna() 우선 3일간의 X 값과 모델 1에서 연산한 (라)값을 합하여 X 를 구성하였습니다. y 는 3일 간의 Y18 온도 센서 측정 값입니다. LSTM 모델을 세팅합니다. 1234567K.clear_session()model_2 = Sequential() # Sequeatial Modelmodel_2.add(LSTM(20, input_shape=(X.shape[1], 1)))model_2.add(Dense(1)) # output = 1model_2.compile(loss='mean_squared_error', optimizer='adam')model_2.summary() 모델 1과 동일하나, y18 의 크기를 고려하여 output 데이터의 크기를 1로 조정했습니다. 모델 1과 동일하게 세팅한 학습 데이터의 크기에 맞게 X 를 조정하겠습니다. 12X = X.valuesX = X.reshape(X.shape[0], 58, 1) 이때, X 의 y값은 40개의 기상청 데이터와 18개의 온도 측정 센서를 더한 58로 설정합니다. 모델 학습을 진행하겠습니다. 1234early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1)model_2.fit(X, y, epochs=10000, batch_size=30, verbose=1, callbacks=[early_stop]) 최종 loss값 6.6453으로 학습을 마무리했습니다. 모델2 역시 기본 아이디어에 따라 이후 사용됩니다. 재사용의 편의를 위해 모델을 저장하고 적용 함수를 만들었습니다. 123456789101112joblib.dump(model_2, 'model_2.pkl')# 모델2 적용 함수def model_fit_2(data1, data2, model_name, file_name): data=pd.concat([data1.reset_index(drop=True), data2.reset_index(drop=True)], axis=1) model = joblib.load(str(model_name)) data_r = data.values.reshape(data.shape[0],data.shape[1],1 ) pred_out=model.predict(data_r) df = pd.DataFrame({'id':range(144*33, 144*113), 'Y18':pred_out.reshape(1,-1)[0]}) df.to_csv(file_name, index = False) return df 3. 평가본 대회의 평가지표는 MSE입니다. MSE관련 내용은 다른 포스트에서 소개하겠습니다. 만든 모델을 평가하겠습니다. 80일 간의 기상청 데이터 값인 test 값으로 제출 파일을 만들었습니다. 1234# 모델1 : X (자) → y (사) 예측pred_out_1 = model_fit_1(test, 'model_1.pkl', 'pred_out_1.csv')# 모델2 : X (자+사) → y (아) 예측pred_out_fin = model_fit_2(test, pred_out_1, 'model_2.pkl', 'pred_out_fin.csv') 제출 결과, 8.9381423723점으로 약 9점에 가까운 점수를 받았습니다. 데이터 전처리 없이 기본 LSTM 모델로 학습을 진행한 데모이기에, 9점이 좋은 모델의 지표가 될 수 있을거라 생각됩니다. 관련 카테고리 포스트 더보기 Machine-Learning 관련 포스트 더보기 Dacon 관련 포스트 더보기 Dacon 온도추정 대회 관련 포스트 더보기","link":"/2020/03/15/temperature-forecast-02/"},{"title":"시계열 데이터를 다뤄보자 - 비트코인 시세 예측하기","text":"시간 축과 데이터로 이루어진 시계열 데이터, 그 중 불규칙한 시계열 데이터를 살펴보고 ARIMA 모델로 트렌드 예측을 진행합니다. 비트코인 시세 데이터를 활용하여 시간 축과 데이터로 이루어진 시계열 데이터, 그 중 불규칙한 시계열 데이터를 살펴보고 ARIMA 모델로 트렌드 예측을 진행합니다. 0. 시계열 데이터시계열 데이터는 연속적인 시간에 따라 다르게 측정되는 데이터입니다. 즉, 관측치가 시간적 순서를 가진 데이터로 변수간의 상관성이 존재하는 데이터를 의미합니다. 규칙적 시계열 데이터를 분석 (예. 심장박동 데이터) 불규칙적 시계열 데이터를 분석 (예. 비트코인 시세 예측) 시계열 데이터는 과거의 데이터를 통해서 현재의 동향이나 미래를 예측하는데 사용됩니다. 과거의 특정 구간대의 데이터를 통해 미래를 예측할 수 있다는 것입니다. 이번 포스팅에서는 3년간 비트코인 시세를 바탕으로 일주일 간의 비트코인 시세를 예측해보려고 합니다. blockchain에서 제공하는 데이터를 재가공하여 사용하였으며, 원본 데이터셋은 링크에서 다운로드 받을 수 있습니다. 간혹 확장자가 없는 파일로 다운로드되는데, 확장자 .csv를 붙여주기만 하면 사용할 수 있습니다. 본 포스팅에서 사용하는 학습 데이터는 3년간 시세 데이터(train.csv) , 테스트 데이터는 7일간 시세 데이터(test.csv)를 사용하였습니다. 데이터셋 환경 구축이 완료되었다면 데이터를 로드해보겠습니다. 1. 데이터 로드train데이터를 살펴보면 결측값 없이 1091개의 행과 2개의 칼럼으로 구성되어 있습니다. 2개의 칼럼은 각각 time과 price로 시계열 데이터 구조입니다. 12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as plt# 3년치 데이터 로드train_path = 'train.csv'train = pd.read_csv(train_path, skiprows = [0], names=['time', 'price'] )train.info() 1234567&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 1091 entries, 0 to 1090Data columns (total 2 columns):time 1091 non-null objectprice 1091 non-null float64dtypes: float64(1), object(1)memory usage: 17.2+ KB 이제 시계열 정보를 to_datetime메소드를 사용하여 DataFrame의 index로 설정합니다. 12345# to_datetime 메소드를 통해 day 피처를 시계열 피처 처리train['time'] = pd.to_datetime(train['time'])# 프레임 인덱스 설정train.index = train['time']train.set_index('time', inplace=True) Pandas에서 시계열 자료를 생성하기 위해서는 DatetimeIndex 자료형으로 데이터 구조를 조작해야 합니다. to_datetime 메소드를 사용하면 날짜 또는 시간을 나타내는 문자열을 자동으로 datetime 자료형으로 바꾼 후 인덱스를 생성할 수 있습니다. 시계열 피처를 처리한 후 시각화 한 결과입니다. 123# 시각화train.plot()plt.show() 2. ARIMA 모델2.1. 모델 소개ARIMA 분석 방법은 크게 두가지 개념을 포함하고 있습니다. AR 모델 : 현재의 상태는 이전의 상태를 참고해서 계산된다는 전제 하자기 자신의 과거를 정보로 사용하는 개념 MA 모델 : 이전 항에서의 오차를 이용하여 현재 항의 상태를 추론하겠다는 방법 이 둘을 합친 모델을 ARMA 모델이라고 하며, 이에 추세 변동의 경향성까지 반영한 모델이 ARIMA 모델입니다. ARIMA 모델의 특징은 선형관계(Correlation)뿐 아니라 추세관계(Cointegtation)까지 고려한 모델이라는 점입니다. 선형관계는 현재의 관계를 의미하고, 추세관계는 과거 현상에서 유추할 수 있는 미래의 수치 간 관계를 의미합니다. 변수 X와 Y간 관계로 간단하게 설명하면 아래의 표와 같습니다. 구분 Cointegtation &gt; 0 Cointegtation &lt; 0 Correlation &gt; 0 X가 양의 값이고 증가하는 추세일 때, Y는 양의 값이고 증가하는 추세 X가 양의 값이고 증가하는 추세일 때, Y는 양의 값이고 감소하는 추세 Correlation &lt; 0 X가 양의 값이고 증가하는 추세일 때, Y는 음의 값이고 증가하는 추세 X가 양의 값이고 증가하는 추세일 때, Y는 음의 값이고 감소하는 추세 2.3 파라미터 조정파이썬에서는 statsmodel 모듈로 ARIMA 분석을 할 수 있습니다. ARIMA 모델을 구성할 때 클래스에 order = (p, d, q) 파라미터 값을 입력해주어야 하는데 적절한 값을 탐색해보겠습니다. 이때 p는 AR이 몇 번째 과거까지 바라보는지에 대한 파라미터, d는 차분에 대한 파라미터, q는 MA가 몇번째 과거까지 바라보는지에 대한 파라미터를 의미합니다. (p, d, q)의 최적 조합을 찾기 위해 plot_acf, plot_pacf 결과를 살펴보겠습니다. ACF : 관측치들 사이 관련성을 측정 PACF : K 이외의 모든 다른 시점 관측치의 영향력을 배제하고 일정 시점의 주 관측치의 관련성을 측정 시계열 데이터가 AR의 측성을 띄는 경우, ACF는 천천히 감소하고 PACF는 처음 시차를 제외하고 급격히 감소합니다. MA의 특성을 띄는 경우 AR과 반대로 ACF 값은 급격히 감소, PACF는 천천히 감소하는 경향을 보입니다. 123456#ARIMA 의 order 파라미터 p,d,q의 최적 조합 찾기from statsmodels.graphics.tsaplots import plot_acf, plot_pacfplot_acf(train)plot_pacf(train)plt.show() ACF 결과를 살펴보면, 자기상관은 항상 양의 값을 가지고 있습니다. p의 값은 크게 조절할 필요가 없다고 판단됩니다. PACF 결과는 처음 시차를 제외하고 급격히 감소합니다. 종합적으로 고려하면 p=0, q=1로 파라미터를 조정하는 것이 적절하다고 판단됩니다. 참고로, p, d, q의 파라미터는 일반적인 가이드라인이 존재합니다. 보통 p와 q의 합이 2미만이거나, p와 q의 곱이 0을 포합한 짝수가 좋은 조합이라고 알려져 있습니다. 다음은 차분 차수를 계산하기 위해, 우선 1차 차분 후 다시 acf, pacf 를 수행했습니다. 123456# 적절 차분 차수 d 계산 tmp=train.diff(periods=1).iloc[1:]tmp.plot()plot_acf(tmp)plot_pacf(tmp)plt.show() 차분이란 현재 상태의 변수에서 바로 전 상태의 변수를 빼주는 것을 의미하며, 시계열 데이터의 불규칙성을 조금이나마 보정해주는 역할을 수행합니다. 또한 ARIMA의 경향성을 반영한 값입니다. 위 결과를 바탕으로 최종 파라미터는 (0,2,1)로 결정하였습니다. 2.3. 모델학습ARIMA 모델을 활용하여 모델 학습을 수행하겠습니다. 123456from statsmodels.tsa.arima_model import ARIMAimport statsmodels.api as smmodel = ARIMA(train.price.values, order=(0,2,1)) #파라미터 설정model_fit = model.fit(trend='c', full_output=True, disp=True)print(model_fit.summary()) 1234567891011121314151617181920ARIMA Model Results ==============================================================================Dep. Variable: D2.y No. Observations: 1089Model: ARIMA(0, 2, 1) Log Likelihood -8038.507Method: css-mle S.D. of innovations 387.378Date: Sat, 25 Apr 2020 AIC 16083.015Time: 20:03:06 BIC 16097.994Sample: 2 HQIC 16088.684 ============================================================================== coef std err z P&gt;|z| [0.025 0.975]------------------------------------------------------------------------------const -0.0285 0.037 -0.763 0.445 -0.102 0.045ma.L1.D2.y -1.0000 0.003 -356.421 0.000 -1.005 -0.995 Roots ============================================================================= Real Imaginary Modulus Frequency-----------------------------------------------------------------------------MA.1 1.0000 +0.0000j 1.0000 0.0000----------------------------------------------------------------------------- ARIMA 모델의 t-test 값은 p-value로 상수항을 제외한 모든 계수의 p-value가 0.05이하인 경우 유의미합니다. ma.L1.D2.y 변수값의 p-value는 0.000으로 적정한 파라미터 값을 대입했다고 판단됩니다. 학습한 모델에 학습 데이터 셋을 넣었을 때 시계열 예측 결과입니다. plot_predict() 메소드로 시각화를 자동으로 수행할 수 있습니다. 12# 학습 데이터 예측 결과 -&gt; 양호fig = model_fit.plot_predict() 다음은 실제와 예측값 사이 오차 변동(잔차)을 살펴보겠습니다. 잔차의 폭이 일정할 경우 바람직한 학습 결과를 기대할 수 있습니다. 1234# 잔차의 변동 시각화residuals = pd.DataFrame(model_fit.resid)residuals.plot()# 폭이 일정해야 좋음 -&gt; 불규칙한 형태 하지만 실행 결과에서는 상당히 불안정한 잔차값이 나타납니다. 그래도 모델을 추가 수정하지 않고 평가 단계로 넘어가겠습니다. 3. 평가3.1. 예측 결과모델을 평가하기 위해서 가장 최근 7일치 데이터를 사용하므로, 학습한 모델에서 7일치의 데이터 연산을 진행합니다. 12pred_model = model_fit.forecast(steps=7) #7일 예측pred=pred_model[0].tolist() 예측값은 변수 pred에 list형태로 저장했습니다. 3.2. 시각화실제 데이터를 로드하고 예측결과와 비교하기 위해 시각화합니다. 예측 최고가격과 최저가격을 설정한 후 실제 가격과 예측 가격을 비교하겠습니다. 1234plt.plot(pred, color=&quot;blue&quot;) # 예측 가격plt.plot(pred_lower, color = &quot;gray&quot;) # 예측 최저가격plt.plot(pred_upper, color = &quot;pink&quot;) # 예측 최고가격plt.plot(test, color = &quot;red&quot;) # 실제가격 예측 가격과 실제 가격의 추이가 비슷한 양상을 띄는 것을 확인할 수 있습니다. 이번에는 더욱 자세히 비교하기 위해 최저, 최고 가격을 제외하고 예측 가격과 실제 가격만의 그래프를 확인해보겠습니다. 123test=test.price.values # 데이터 형식 list로 변경plt.plot(pred, color=&quot;blue&quot;) # 예측 가격plt.plot(test_y, color = &quot;red&quot;) # 실제가격 세부 결과는 좋지 않았습니다. 예측 가격은 7일동안 낮아질 것으로 나타났으나, 실제 데이터는 등락을 반복하는 경향을 보였습니다. 그러나 최고, 최저 예측 가격의 폭이 약 4000인 것을 고려한다면 600정도(0.15)의 오차는 그렇게 큰 편은 아니라고 판단됩니다. 3.3. 모델평가rmse를 사용하여 모델을 평가한 결과입니다. 12345from sklearn.metrics import mean_squared_errorimport mathrmse = math.sqrt(mean_squared_error(pred, test_y))print(rmse) 1270.8342603398055 rmse값은 270으로 처음 감소 추세는 예측 성공하였으나 큰 폭으로 감소하는 결과값을 정확하게 예측하기에는 어려웠습니다. 4. 결론불규칙한 시계열 예측의 경우, 먼 미래를 예측하는 것은 큰 의미가 없습니다. 따라서 앞으로 N일 동안 어느 정도로 상승(하락)할 것인지 대략적인 Trend 예측만을 수행하는 것이 일반적입니다. 참고자료 : 이것이 데이터 분석이다(윤기태 저) 관련 카테고리 포스트 더보기 Machine-Learning 관련 포스트 더보기","link":"/2020/03/15/time-series/"},{"title":"베이지안 최적화로 하이퍼파라미터 수정하기 (Bayesian Optimization)","text":"이번 포스트에서는 Auto ML 방법 중 하이퍼 파라미터 튜닝에 대해 다루며, Bayesian Optimization(베이지안 최적화) 방법을 소개하겠습니다. 📝 Bayesian Optimization베이지안 최적화는 Auto ML 분야에서도 Hyperparameter Optimization(하이퍼 파라미터 튜닝)에 대한 내용입니다. Auto ML우선 Auto ML 분야에 대해 알아보겠습니다. Auto ML은 ‘머신러닝으로 설계하는 머신러닝’으로, 학습 데이터가 정형 데이터일 때 Auto ML 기술을 적용하면 적은 노력으로 최적의 결과를 도출할 수 있습니다. Auto ML은 크게 3가지로 나뉩니다. Automated Feature Learning 입력 데이터 중 유의미한 피처를 추출하여 입력으로 사용하는 방법입니다. 최적의 피처 추출 방법을 학습을 통해 찾을 수 있습니다. Architecture Search 학습을 통해 최적의 아키텍처를 설계하는 방법입니다. 모델의 구조적 측면을 다루며 Darts 등이 이에 해당합니다. Hyperparameter Optimization 학습을 시키기 위해 필요한 하이퍼파라미터를 학습을 통해 추정합니다. Auto ML 패키지인 PyCaret 을 사용한다면 위 모든 과정을 한꺼번에 진행할 수 있지만, 이번 포스트에서는 Hyperparameter Optimization 위주로 다루겠습니다. Hyperparameter Optimization하이퍼 파라미터 튜닝은 학습을 수행하기 위해 사전에 설정해야 하는 파라미터의 최적값을 탐색하는 문제입니다. 우선 파라미터는 모델 학습 시 필요한 여러 설정값입니다. 이 파라미터의 값에 따라 학습 결과에 큰 영향을 미칠 수 있습니다. 이러한 값들의 최적 조합을 뽑아내는 방법론이 Hyperparameter입니다. Auto ML 을 적용하면 학습률(learning rate), 배치 크기(batch size) 등 학습에 영향을 주는 하이퍼파라미터들을 기존 수동적 조정에서 나아가 학습을 통한 최적의 하이퍼 파라미터를 추정할 수 있습니다. 먼저 기존 파라미터 탐색 방법을 소개하겠습니다. Maual SearchMaual Search 는 수동적으로 파라미터 값을 탐색하는 방법입니다. 여러번의 탐색 과정 중 가장 좋은 결과값을 선택하는 방법으로 주관과 직관에 기반합니다. 이 방법은 실험을 통해 도출된 결과값이 실제 최적값인지 의문을 해소하기 어렵습니다. 나아가 한번에 하나의 파라미터를 추정하는 것이 아니라, 일반적으로 한번에 여러 종류의 파라미터를 동시에 탐색하는데 이러한 경우 파라미터 간 상호 연관 관계를 무시할 수 없기에 더욱 복잡한 연산을 수행해야 합니다. 예시로 Learning rate와 L2 정규화 계수는 서로 상관 관계가 있는 파라미터로 Maual Search 방법으로 조정하기가 어렵습니다. Grid SearchGrid Search 는 Maual Search 의 단점을 보완하여 탐색 구간 내 추정하고자 하는 하이퍼파라미터 값 들을 일정한 간격을 두고 선정하여 가장 높은 성능을 발휘했던 하이퍼파라미터 값을 최종 선정하는 방법입니다. 물론 전체 탐색 대상 구간의 설정 방법, 간격의 길이 설정 방법 등 수동적인 요소는 남아있지만, 균등하고 전역적인 탐색이 가능합니다. 그러나 추정하고자 하는 하이퍼 파라미터 개수를 늘리게 되면 탐색해야 할 구간도 비례하여 증가하므로 탐색 연산 비용이 기하급수적으로 증가하게 됩니다. Random SearchRandom Search는 Grid Search와 비슷한 맥락으로 탐색 대상 구간 내 하이퍼 파라미터 값들을 랜덤 샘플링을 통해 선정합니다. 즉, 성능 함수의 최댓값이 예측되는 구간에 파라미터 조합을 랜덤으로 샘플링하여 최적 조합을 탐색하는 방법입니다. Grid Search에 비해 불필요한 반복 수행 횟수를 줄일 수 있다는 장점이 있습니다. 정리Random Search와 Grid Search는 하이퍼파라미터 값들의 성능 결과에 대한 이전의 학습 결과가 반영되지 않습니다. Maual Search의 경우 이전의 학습 결과를 바탕으로 수동으로 조합 값들을 조정했지만 Random Search와 Grid Search는 아직도 불필요한 탐색이 반복됩니다. 이제부터 소개할 Bayesian Optimization방법은 이와 다르게 이전의 학습 결과를 반영하여 최적 조합을 탐색할 수 있습니다. Bayesian Optimization베이지안 최적화 방법은 목적함수 f에 대해 함수값 f(x)를 최대로 만드는 최적해 x를 탐색하는 방법입니다. 이때 목적함수 f는 표현식을 명시적으로 알지 못하는 black box fuction 이며 하나의 함수값을 계산하는 데 오랜 시간이 소요된다고 가정합니다. 따라서 가능 한 적은 수의 x 후보에 대해서만 함수값을 연산하며 f를 최대로 만드는 최적해 x를 빠르고 효과적으로 탐색하는 것이 목적입니다. 이 목적함수 f는 모델의 성능 함수 f, x는 하이퍼 파라미터의 조합으로 생각하면 보다 쉽게 이해할 수 있습니다. 목적함수 f를 추정하기 위해서는 이전의 학습 결과가 반영된 모델(Surrogate Model) 과 다음 입력값 x 후보를 추천해주는 함수(Acquisition Function) 이 필요합니다. Surrogate Model : 현재까지 조사된 입력값-함숫값 점들( (x1, f(x1)) (x2, f(x2) …), 을 바탕으로 f를 추정하는 확률모델로 Gaussian Process 가 주로 사용됩니다. Acquisition Function : f에 대한 Surrogate Model 결과를 바탕으로 다음 최적 입력값 x를 찾기 위해 다음 입력값 후보 x i+1를 탐색하는 함수입니다. 이를 수도코드로 표현하면 다음과 같습니다. 12345678for i=1, 2, 3 … do surrogate model 의 확률적 추적 결과 바탕으로 Acquisition Function를 최대화하는 입력값 후보xi+1설정 f(xi+1) 계산 surrogate model에 ( xi, f(xi+1) )추가하여 확률적 추적 수행 end 먼저 Surrogate Model으로 가장 많이 사용되는 Gaussian Process에 대해 설명하겠습니다. Gaussian Processes (GP)보통의 확률 보델은 특정 변수에 대한 확률 분포를 표현합니다. 이에 반해 Gaussian Processes 는 함수에 대한 확률 분포를 나타내며 각 요소의 결합 분포가 가우시안 분포를 따른다는 특징이 있습니다. f(x)∼GP(μ(x),k(x,x′)) 위 식과 같이 평균 함수 μ와 공분산 함수 k를 사용하여 함수들에 대한 확률 분포를 표현합니다. 사진으로 각 함수의 역할을 자세히 살펴보겠습니다. 위 사진은 t=2,3,4… 에 따라 GP의 연산 과정입니다. 초록색 음영은 Acquisition Function으로 다음 입력값 x를 탐색하고, 보라색 음영과 검정색 실선은 Gaussian Processes으로 f를 추정합니다. 검정색 점선은 실제 f 값인 미지의 목적 함수입니다. 우선, t=2에서 Acquisition Function을 통해 다음 입력값 x(🔻으로 표시)를 탐색합니다. t=3에서 탐색된 입력값 x의 함수값(🔴)을 계산한 후, 다음 입력값 x를 재 탐색합니다. 해당 과정을 반복하여 f의 최대값을 추정할 수 있습니다. Gaussian Processes 부분을 자세히 살펴보겠습니다. 중앙 검정색 실선은 입력값 x에 대한 평균값 μ(x)이고, 검정색 실선을 둘러싼 보라색 음영은 x 위치 별 표준편차 σ(x)입니다. σ(x)의 값을 살펴보면, 조사된 점(위 사진의 observation(x))의 값에서 멀어질수록 σ(x)이 크게 나타납니다(t=2에서 보라색 음영이 확장). 즉, 추정한 평균값 μ(x)의 불확실성이 크다는 뜻입니다. 거듭 연산이 진행될수록 f의 추정 결과가 압축됩니다(t=4 에서 보라색 음영의 축소). 즉, 조사된 점의 개수가 늘어날수록 평균값 μ(x)의 불확실성이 감소됩니다. 불확실성이 감소될수록 최적 입력값 x를 찾을 가능성이 높아집니다. Acquisition FunctionAcquisition Function은 f에 대해 확률 추정 모델(GP 등)의 결과를 바탕으로 t+1 번째 입력값 후보 x i+1 를 추천하는 함수입니다. 이때 최적 입력값 x는 현재까지 조사된 점들 중 함수값이 최대인 점 근방에 위치하거나, 표준편차가 최대인 점 근방(불확실한 영역)에 위치할 가능성이 높습니다. 이 두 가지 경우를 각각 exploitation, exploration 전략이라 합니다. 이러한 경우의 수를 고려하여 x i+1을 탐색해야 합니다. exploitation 현재까지 조사된 점들 중 함수값이 최대인 점 근방을 다음 차례에 시도합니다. 함수값이 가장 큰 점 근방에서 실제 최적 입력값 x를 찾을 가능성이 높기 때문입니다. exploration 현재까지 추정된 목적 함수 상에서 표준편차가 최대인 점 근방을 다음 차례에 시도합니다. 불확실한 영역에 최적 입력값 x이 존재할 가능성이 높기 때문입니다. Acquisition Function으로 가장 많이 사용되는 함수는 exploitation, exploration 전략 모두를 사용하는 Expected Improvement 입니다. Expected Improvement(EI)Expected Improvement는 현재까지 추정된 f를 바탕으로 어떤 입력값 x에 대해서 현재까지 조사된 점들의 최대 함수값 f(x+) 보다 큰 함수값을 도출할 확률과 그 함수값과 최대 함수값 f(x+) 간 차이값을 고려하여 x의 유용성을 반환합니다. 아래 그래프로 자세히 살펴보겠습니다. 위 그래프는 x+ 이 계산된 f에 대해 다음 후보 입력값인 x1, x2,x3 중 가장 유용한 값을 탐색하는 과정입니다. 오른쪽의 초록색 음영은 최대 함수값 f(x+) 보다 큰 함수값을 도출할 확률 PI(x3) 로, 다음 입력값으로 x3을 채택하는 것이 가장 유용하다고 판단됩니다. 따라서 PI(x3)값에 함수값 f(x3)에 대한 평균(검정색 실선에 해당하는 μ(x3)값) 과 f(x3)-f(x+)을 가중하여 최종 EI를 계산합니다. 이 계산 과정을 통해 f(x+)보다 큰 함수값을 도출할 수 있는 가능성 뿐 아니라 실제로 f(x3)값이 f(x+)보다 얼마나 더 큰 값인지도 반영할 수 있습니다. 수식으로 살펴보겠습니다.$$\\begin{align}EI(x) &amp; = \\mathbb{E} [\\max (f(x) - f(x^{+}), 0)] \\ &amp; =\\begin{cases} (\\mu(\\boldsymbol{x}) - f(\\boldsymbol{x}^{+})-\\xi)\\Phi(Z) + \\sigma(\\boldsymbol{x})\\phi(Z) &amp; \\text{if}\\ \\sigma(\\boldsymbol{x}) &gt; 0 \\ 0 &amp; \\text{if}\\ \\sigma(\\boldsymbol{x}) = 0\\end{cases}\\end{align}$$ $$Z =\\begin{cases} \\frac{\\mu(\\boldsymbol{x})-f(\\boldsymbol{x}^{+})-\\xi}{\\sigma(\\boldsymbol{x})} &amp; \\text{if}\\ \\sigma(\\boldsymbol{x}) &gt; 0 \\ 0 &amp; \\text{if}\\ \\sigma(\\boldsymbol{x}) = 0\\end{cases}$$ EI 계산식 Φ : 표준정규분포의 누적분포함수(CDF) ϕ : 표준정규분포의 확률분포함수(PDF) ξ : exploration과 exploitation 간의 상대적 강도를 조절해 주는 파라미터 클수록 exploration의 강도가 높아짐 작을수록 exploitation의 강도가 높아짐 결론지금까지 살펴본 Bayesian Optimization 의 수행 과정을 살펴보겠습니다. 12345678910111213141516171819202122232425261. 입력값, 목적 함수 및 그 외 설정값들 정의 - 입력값 x: 학습률 - 목적 함수 f(x) : 성능 함수 (e.g. 정확도) - 입력값 x의 탐색 대상 구간: (a,b). - 맨 처음에 조사할 입력값-함숫값 점들의 갯수: n - 맨 마지막 차례까지 조사할 입력값-함숫값 점들의 최대 갯수: N 2. 설정한 탐색 대상 구간내에서 처음 n개의 입력값들을 랜덤하게 샘플링하여 선택3. 선택한 n개의 입력값 x1,x2,..,xn을 각각 학습률 값으로 설정4. 딥러닝 모델을 학습한 뒤, 검증 데이터셋을 사용하여 학습이 완료된 모델의 성능 결과 수치를 계산 - 이들을 각각 함숫값 f(x1),f(x2),...,f(xn)으로 간주 5. 입력값-함숫값 점들의 모음에 대하여 Surrogate Model로 확률적 추정 수행6. 조사된 입력값-함숫값 점들이 총 N개에 도달할 때까지 반복 - 기존 입력값-함숫값 점들의 모음에 대한 Surrogate Model의 확률적 추정 결과를 바탕으로, 입력값 구간 (a,b)(a,b) 내에서의 EI의 값을 계산 - 그 값이 가장 큰 점을 다음 입력값 후보 xt+1로 선정 - 다음 입력값 후보 xt+1을 학습률 값으로 설정하여 딥러닝 모델을 학습한 뒤, 검증 데이터셋을 사용하여 학습이 완료된 모델의 성능 결과 수치 계산 → f(xt+1)값 - 새로운 점 (xt+1,f(xt+1))을 기존 입력값-함숫값 점들의 모음에 추가 - 갱신된 점들의 모음에 대하여 Surrogate Model로 확률적 추정을 다시 수행 7. 총 N개의 입력값-함숫값 점들에 대하여 확률적으로 추정된 목적 함수 결과물을 바탕으로, 평균 함수 μ(x)을 최대로 만드는 최적해 x∗를 최종 선택8. 해당 x∗ 값을 학습률로 사용하여 딥러닝 모델을 학습하면, 일반화 성능이 극대화된 모델을 얻을 수 있음 📗 정리이상으로 Bayesian Optimization에 대해 살펴보았습니다. 실제 적용에 대해서는 데이콘의 심리성향 예측 대회 데이터를 바탕으로 다루겠습니다. 아래 링크에서 확인할 수 있습니다. [데이콘 심리성향 예측 대회] AUTO ML - 베이지안 최적화 (Bayesian Optimization) 적용하기 Reference Shahriari et al., Taking the human out of the loop: A review of bayesian optimization. Shahriari, Bobak, et al. “Taking the human out of the loop: A review of bayesian optimization.” Proceedings of the IEEE 104.1 (2016): 148-175. Brochu et al., A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. Brochu, Eric, Vlad M. Cora, and Nando De Freitas. “A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning.” arXiv preprint arXiv:1012.2599 (2010). Bengio et al., Practical recommendations for gradient-based training of deep architectures. Bengio, Yoshua. “Practical recommendations for gradient-based training of deep architectures.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 437-478. Goodfellow et al., Deep learning. Goodfellow, Ian, et al. Deep learning. Vol. 1. Cambridge: MIT press, 2016. Bergstra and Bengio, Random search for hyper-parameter optimization. Bergstra, James, and Yoshua Bengio. “Random search for hyper-parameter optimization.” Journal of Machine Learning Research 13.Feb (2012): 281-305. Fernando Nogueira, bayesian-optimization: A Python implementation of global optimization with gaussian processes. https://github.com/fmfn/BayesianOptimization Hunting Optima, Expected Improvement for Bayesian Optimization: A Derivation. http://ash-aldujaili.github.io/blog/2018/02/01/ei/ 관련 카테고리 포스트 더보기 Machine-Learning 관련 포스트 더보기","link":"/2020/10/07/Bayesian-Optimization/"}],"tags":[{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"Dacon","slug":"Dacon","link":"/tags/Dacon/"},{"name":"temperature-forecast","slug":"temperature-forecast","link":"/tags/temperature-forecast/"},{"name":"time-serise","slug":"time-serise","link":"/tags/time-serise/"},{"name":"bitcoin","slug":"bitcoin","link":"/tags/bitcoin/"},{"name":"Bayesian-Optimization","slug":"Bayesian-Optimization","link":"/tags/Bayesian-Optimization/"},{"name":"Hyperparameter","slug":"Hyperparameter","link":"/tags/Hyperparameter/"}],"categories":[{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"leetcode","slug":"Algorithm/leetcode","link":"/categories/Algorithm/leetcode/"},{"name":"Machine-Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Dacon","slug":"Machine-Learning/Dacon","link":"/categories/Machine-Learning/Dacon/"}]}