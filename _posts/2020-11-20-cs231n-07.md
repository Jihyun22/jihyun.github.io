---
title: "[CS231n] Lecture 7 - Training Neural Networks, Part 2"
date: 2020-11-20
categories: dl
toc: true
toc_sticky: true
toc_label: "목차"
tags : data dl datastudy cs231n
related: true
header:
  teaser: "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/cs231n.png?raw=true"
---



<img src= "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/teasers/cs231n.png?raw=true">

스탠포드 대학의 CS231n 강의노트입니다. 

CS231n강의는 1~16강으로 구성되어 있으며 이번 포스트는 **Lecture 7 - Training Neural Networks, Part 2"** 에 대해 다루겠습니다.

*가짜연구소*의 *딥러닝 이론 스터디* 활동으로 작성되었습니다.

- youtube 강의 영상 바로가기 [링크](https://youtu.be/vT1JzLTH4G4?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk)
- 가짜연구소 사이트 바로가기 [링크](https://pseudo-lab.com/)

<center><p>
  <img src="https://visitor-badge.laobi.icu/badge?page_id=jihyun22.github.io/dl/cs231n-07/" alt="visitor"/>
    </p></center>







---

<br/>



# 🚀 **개요** 

---

<br/>

지난 6장까지 뉴런 네트워크, 역전파, 최적화, 활성 함수 등을 학습했습니다. 또하 하이퍼파라미터를 찾기 위한 grid search 와 random search 도 다루었습니다.

이번 7장에서는 6장에서 다루었던 내용의 확장으로 최적화와 정규화를 다룰 예정입니다. 또한 배치 정규화를 통해 drop out 기법을 더욱 자세히 알아보겠습니다.

<br/>



---

## 최적화

---

vanilla gradient descent 를 하는 경우, 아래 슬라이드 처럼 가중치를 초기화하는 과정이 필요합니다. 이때, 배치 단위로 끊어 다루는 방법이 SGD입니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/14.jpg?raw=true)

SGD는 특정 경우에 속도가 느린 것이 가장 큰 단점입니다.

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/16.jpg?raw=true)

타원의 모양을 가지는 경우, 빨간색 점이 중앙의 이모티콘이 있는 지점까지 찾아갈 때, x축 방향은 완만하고, y축 방향은 크므로 지그재그 형태로 이동을 하게 됩니다. 따라서 굉장히 비효율적인 추적이 이루어집니다.

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/18.jpg?raw=true)

이러한 문제를 loss가 local minima(위 곡선), sddle point(아래 곡선)에 빠졌다고 말합니다. 

순간적으로 기울가기 0에 가까운 지점이 생기면 멈추게 되는 것입니다. 고차원에서는  sddle point이 더 자주 발생합니다. 

이러한 문제를 해결하기 위해 등장한 개념이 모멘텀(momentum)입니다.

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/21.jpg?raw=true)

물리학적으로 가속도를 주는 개념이라고 생각하면 이해하기 쉽습니다. 기울기가 0인 지점에 빠지더라도 가속도로 인해 탐색을 계속할 수 있습니다.

위 코드에서는 vx 가 속력, rho 가 마찰계수로 사용되었습니다. 보통 rho 는 0.9~0.99값을 지정합니다.

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/22.jpg?raw=true)

모멘텀 개념을 적용한 결과, 기존의 SGD 보다 파란색 곡선으로 효율적인 탐색이 가능함을 확인할 수 있습니다 



## AdaGrad

---

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/26.jpg?raw=true)

<br/>

CNN구조를 이해하기 위해서는 Fully Connected Layer 을 이해해야 합니다.

이 구조는 어떤 벡터를 가지고 연산을 할 때 layer 간 모든 요소의 백터가 서로 연결된 구조를 의미합니다. 이때 activation은 해당 layer의 출력입니다.

<br/>



![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/29.jpg?raw=true)

<br/>

기존의 FC Layer가 입력 이미지를 1차원 벡터로 변형시켜 연산을 진행했다면, 이제는 기존의 이미지 구조를 그대로 유지하여 input 입력 데이터로 사용합니다.

이후 위 슬라이드에서 filter 라는 가중치 벡터를 통과해서 내적 연산을 수행합니다. 

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/31.jpg?raw=true)

<br/>

이때 원본 데이터의 깊이, 3은 전체를 취하지만, 32* 32 중 filter 의 크기인 5*5만 취해서 해당 이미지의 픽셀을 곱해줍니다. 

물론, 각 벡터 간 convolution 을 하거나, 이미지를 1차원으로 펼쳐 내적 연산을 수행하는 것과 동일한 연산입니다.

결과값은 필터당 1개로, 여러개의 필터를 사용하면 여러 activation map을 얻을 수 있습니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/36.jpg?raw=true)

<br/>

각 레이어의 출력은 다음 layer의 입력이 됩니다.

이때 pooling은 activation maps의 사이즈를 줄이는 역할을 수행합니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/37.jpg?raw=true)

필터를 많이 통과할수록, 즉 layer의 계층이 높아질 수록 더 많은 특징을 담아낼 수 있습니다.

<br/>

---

## Adam

---

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/47.jpg?raw=true)

<br/>

다음은 stride의 개념을 알아보겠습니다.

stride는 한 칸씩 움직이는 정도를 의미하며, 이때 stride가 input 이미지에 맞지 않으면 불균형한 결과를 부를 수도 있습니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/52.jpg?raw=true)

<br/>

따라서 해당 필터를 통과하면 얻을 수 있는 output 사이즈는 (input size-filter) / stride +1 입니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/53.jpg?raw=true)

<br/>

추가로, Zero-padding은 가장자리의 필터 연산을 수행하여 입력의 사이즈를 유지하는 역할을 수행합니다. 위와 같은 연산을 수행하면 새 출력이 7*7이 되며 출력의 차원이 입력의 차원과 같아지게 됩니다.

이때, Zero-padding을 하지 않으면, Layer가 쌓이면서 output 사이즈가 급격히 줄어들게 돕니다. 이러한 결과는 activation map이 작아지기 때문에 바람직한 결과가 아닙니다.

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/cs231n-7/60.jpg?raw=true)

실제 계산을 진행하면 위와 같이 계산되며 최종, 파라미터의 개수는 760입니다.



<br/>

---



# **📗 정리**

---

 <br/>

뉴런 네트워크의 활성화함수와 전처리 방법에 대해 다루었습니다.

다음 7장에서는 뉴런 네트워크의 2번째 내용을 정리하도록 하겠습니다.

 <br/>


