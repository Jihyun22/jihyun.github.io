---
title: "[모두를위한딥러닝PyTorch] Lab 01 - Tensor Manipulation"
date: 2021-01-23
categories: dl
toc: true
toc_sticky: true
toc_label: "목차"
tags : data dl datastudy Pytorch
related: true
header:
  teaser: "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/assets/images/logo3.png?raw=true"
---



<img src= "https://github.com/Jihyun22/Jihyun22.github.io/blob/master/assets/images/logo3.png?raw=true">

모두를 위한 딥러닝 시즌2 - PyTorch 리뷰 포스팅입니다.

이번 포스트는 **Lab 01 - Tensor Manipulation** 에 대해 다루겠습니다.

가짜연구소(PseudoLab) 2기 <모두가 한걸음씩 성장하는 Pytorch> 스터디 활동으로 작성되었습니다.

<br/>

- 스터디 노션 페이지 [바로가기](https://www.notion.so/pytorch-e3c977d66a6d412d92a91834f1679409)
- 모두를 위한 딥러닝 시즌2-PyTorch 사이트 [바로가기](https://deeplearningzerotoall.github.io/season2/lec_pytorch.html)
- 강의자료 슬라이드 [바로가기](https://drive.google.com/drive/folders/1qVcF8-tx9LexdDT-IY6qOnHc8ekDoL03)

<br/>

<center><p>
  <img src="https://visitor-badge.laobi.icu/badge?page_id=jihyun22.github.io/dl/cs231n-09/" alt="visitor"/>
    </p></center>
---

<br/>

# 🚀 **개요** 

---

<br/>

1강에서는 Tensor 의 개념을 다지고, Numpy 와 PyTorch Tensor 를 비교하여 알아보겠습니다.

<br/>



---

## Vector, Matrix and Tensor

---

Vector, Matrix, Tensor는 차원을 비교하여 설명할 수 있습니다. 

<br/>

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/Lab01/1/0003.jpg?raw=true)

<br/>

1D, 1차원은 Vector을 의미하며 2차원은 Matrix, 3차원은 Tensor로 말할 수 있습니다.

이때 3차원 이상의 차원은 Tensor 가 여러겹 쌓인 모양으로 표현합니다.

참고로, Vector 이하의 개념으로 Scalar가 존재합니다.

<br/>

딥러닝에서 각 layer의 가중치 등을 계산할 때, 가장 중요한 것이 Matrix 및 Tensor 연산입니다.

연산 시 각각의 크기를 고려해야 하는데, 크기를 표현하는 방식에 대해 아래 슬라이드를 통해 알아보겠습니다.

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/Lab01/1/0004.jpg?raw=true)

<br/>

2차원의 경우 batch size * dim 으로 표현할 수 있습니다.

이때, batch size는 위 도형의 세로 크기, dim 은 가로 크기입니다.

일반적인 모델은 batch size = 64, dim = 256 정도로 설정되어 있습니다.

<br/>



![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/Lab01/1/0005.jpg?raw=true)

<br/>

3차원, 특히 시각과 관련한 모델(컴퓨터 비전)에서의 Tensor의 크기는 batch size * width * height으로 표현할 수 있습니다.

이때, batch size, width , height에 대해서는 아래 그림을 참조하세요

![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa29e65d5-8072-4918-9347-9dbb69429703%2FUntitled.png?table=block&id=6096d3e3-8b73-4739-bf90-c6c32742af73&width=380&userId=cc046dbc-d7dd-4ea9-807e-0e695d764469&cache=v2)

<br/>



![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/Lab01/1/0006.jpg?raw=true)

<br/>

덧붙여 3차원, 특히 자연어 처리와 관련된 모델(NLP)에서의 Tensor의 크기는 batch size * length* dim으로 표현할 수 있습니다.

쉽게 설명하면 한 문장(length*dim)이 모여 batch size 만큼 쌓인 더미가 이 Tensor 라고 볼 수 있습니다.

<br/>

## Numpy 와 Tensor

---

다음으로 `Numpy` 와 `PyTorch` Tensor 를 비교하여 연산 메소드를 비교해보겠습니다.

`PyTorch` Tensor 연산은 `Numpy` 의 array 연산과 많이 닮아 있습니다. 본 포스팅에서는 `numpy` 파트를 제외하고 중요한 연산부 위주로 다루겠습니다.

```python
import torch
```

import 가 정상적으로 되었다는 가정 하에 이하의 코드가 수행됩니다. 

<br/>

### 1D Tensor

---

Tensor 객체를 선언하는 것은 간단합니다. `torch.FloatTensor` 메소드에 파이썬 리스트를 지정해주면 합니다.

```python
# Tensor 1차원 객체 선언
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t) # tensor([0., 1., 2., 3., 4., 5., 6.])
```

간단하게 0부터 6까지의 float 타입의 elements로 구성된 list로 tensor t를 선언했습니다.

<br/>

t의 size를 알아보겠습니다.

```python
print(t.dim())  # 1
print(t.shape)  # torch.Size([7])
print(t.size()) # torch.Size([7])
```

t는 1차원이므로 `.dim `연산시 1이 출력됩니다.

`.shape` 는 `size()` 메소드와 동일하게 dim의 elements 값을 출력할 수 있습니다.

<br/>

tensor 연산은 numpy와 동일하게 slicing 와 index이 가능합니다.

t의 원소는 아래의 코드로 index 될 수 있습니다.

```python
print(t[0], t[1], t[-1])	# tensor(0.) tensor(1.) tensor(6.)
print(t[2:5], t[4:-1])		# tensor([2., 3., 4.]) tensor([4., 5.])
print(t[:2], t[3:])			# tensor([0., 1.]) tensor([3., 4., 5., 6.])
```

<br/>

### 2D Tensor

---

2차원 Tensor 도 동일하게 선언됩니다. 

```python
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                      ])
```

t는 4,3의 크기의 tensor 로 선언되었습니다.

<br/>

새롭게 선언된 t의 size를 알아보겠습니다.

```python
print(t.dim())  # 2
print(t.shape)  # torch.Size([4, 3])
```

t는 2차원이므로 `.dim `연산시 2가 출력됩니다.

`.shape` 는 dim 크기인 (4, 3)을 출력합니다.

<br/>

slicing 연산으로 t의 원소를 조작해보겠습니다.

slicing 에 대해 덧붙이자면, `,` 를 기준으로 앞이 row, 뒤에가 col을 의미하는데 `:` 기호를 통해 전체를 불러올지, 혹은 index 한 원소만 불러올지를 결정하여 객체의 일부를 불러오는 연산입니다.

```python
print(t[:, 1]) 			# tensor([ 2.,  5.,  8., 11.])
print(t[:, 1].size()) 	# torch.Size([4])
print(t[:, :-1])		# tensor([[ 1.,  2.],[ 4.,  5.],[ 7.,  8.],[10., 11.]])
```

`t[:, 1]` 은 t의 두번째(1) 열의 원소인 2, 5, 8, 11을 의미합니다.

총 4개의 원소가 포함되므로 size는 4 입니다.

이때 `t[:, :-1]` 에서는 앞의 row를 전체 다 불러오되(`:`), col은 가장 뒤의 열을 제외하고(`:-1`) 불러올 수 있습니다. 따라서 가장 마지막 row인 3, 5, 9, 12 가 제외된 t가 출력됩니다.

지금까지 간단히 tensor의 차원에 대해 알아보았습니다. 

<br/>

### Matrix Multiplication

---

![](https://github.com/Jihyun22/Jihyun22.github.io/blob/master/_posts/images/Lab01/1/0013.jpg?raw=true)

<br/>

딥러닝에서는 행렬곱(Matrix Multiplication)이 상당히 중요합니다.

<br/>

가중치를 곱하여 다음 layer의 input 으로 보내기 위한 적절한 output을 만드는 조작 과정에서 적용되는 연산으로 행렬 간 크기를 고려하여 연산을 해야 적절한 모델을 구축할 수 있습니다.

<br/>

일반적으로 행렬간 곱셈은 곱하고자 하는 각각의 행렬의 col와 row이 일치해야 합니다. 예를 들면, (2,3) * (3,2) 행렬과 같이 크기를 고려해야 합니다.

<br/>

그러나, 일부 연산에서는 각각의 행렬(혹은 tensor)의 크기가 같지 않을 수도 있습니다. 이때 크기가 같지 않은 두 tensor 간의 계산을 가능케 해주는 연산이 Broadcasting입니다.

<br/>

### Broadcasting

---

동일한 차원에서의 연산과 비교하여 Broadcasting의 역할을 살펴보겠습니다.

```python
# 동일한 차원의 tensor 연산
m1 = torch.FloatTensor([[3, 3]]) # m1의 크기는 1,2
m2 = torch.FloatTensor([[2, 2]]) # m2의 크기는 1,2
print(m1 + m2) # tensor([[5., 5.]])
```

<br/>

이때 동일하지 않은 차원의 연산도 살펴보겠습니다.

```python
m1 = torch.FloatTensor([[1, 2]])  # (1, 2)
m2 = torch.FloatTensor([3])  # (1,) --> (1, 2)
print(m1 + m2) # [4,5] 
```

Vector + scalar의 연산입니다. 이때 scalar m2는 자동으로 크기가 큰 vector m1 크기에 맞추어 (1,2)로 조정됩니다. 즉, [[1,2]] + [[3,3]] 의 연산 결과로 [4,5] 가 출력됩니다.

<br/>

차례로 mul, mean, sum, max, argmax의 연산에서 Broadcasting 결과를 살펴보겠습니다.

#### mul

```python
m1 = torch.FloatTensor([[1, 2], [3, 4]])  # 2 x 2
m2 = torch.FloatTensor([[1], [2]])  # 2 x 1

print(m1.matmul(m2))  # tensor([[ 5.], [11.]])
print(m1 * m2)  	# tensor([[1., 2.], [6., 8.]])
print(m1.mul(m2))   # tensor([[1., 2.], [6., 8.]])
```

`m1.matmul(m2)` 은 행렬곱의 결과로 2x1의 결과가 출력됩니다. 

`m1 * m2`, `m1.mul(m2)` 결과는 m2 가 Broadcasting 되어 2x2 크기의 결과가 출력됩니다.

<br/>

#### Mean

```python
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t.mean()) # tensor(2.5000) 
print(t.mean(dim=0)) # tensor([2., 3.])
print(t.mean(dim=1)) # tensor([1.5000, 3.5000])
print(t.mean(dim=-1))# tensor([1.5000, 3.5000])
```

이때, mean의 option 인 `dim=` 은 numpy의 `axis` 과 동일한 역할을 수행합니다.



<br/>











<br/>

---



# **📗 정리**

---

 <br/>

지금까지 CNN의 발전 과정과 모델별 구조를 살펴봤습니다.

다음 강의에서는 이러한 구조적 특징을 고려한 CNN의 파라미터에 대해 알아보겠습니다.

 <br/>


